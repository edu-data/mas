"""
ğŸ“ Discourse Analyzer - ë°œí™” ë‚´ìš© êµìœ¡í•™ì  ë¶„ì„
v5.0: Gemini LLM ê¸°ë°˜ ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜, í”¼ë“œë°± í’ˆì§ˆ ë¶„ì„, Bloom ì¸ì§€ìˆ˜ì¤€ ì¸¡ì •
"""

import os
import re
from typing import Dict, List, Optional

try:
    import google.generativeai as genai
    HAS_GENAI = True
except ImportError:
    HAS_GENAI = False


class DiscourseAnalyzer:
    """
    ğŸ“ ë°œí™” ë‚´ìš© êµìœ¡í•™ì  ë¶„ì„

    LLM ê¸°ë°˜ ë¶„ì„:
    - ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ (ê°œë°©í˜•/íì‡„í˜•/ìˆ˜ì‚¬ì /ìŠ¤ìºí´ë”©)
    - í”¼ë“œë°± í’ˆì§ˆ (êµ¬ì²´ì  ì¹­ì°¬/êµì •ì /ì¼ë°˜ì )
    - Bloom ì¸ì§€ìˆ˜ì¤€ ë¶„í¬
    - ìƒí˜¸ì‘ìš© í’ˆì§ˆ ì ìˆ˜

    í´ë°±: ê·œì¹™ ê¸°ë°˜ íŒ¨í„´ ë§¤ì¹­
    """

    def __init__(self, use_llm: bool = True):
        self.use_llm = use_llm and HAS_GENAI and os.getenv("GOOGLE_API_KEY")
        if self.use_llm:
            genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))

    def analyze(self, transcript: str, segments: List[Dict] = None,
                speaker_segments: List[Dict] = None) -> Dict:
        """
        ë°œí™” í…ìŠ¤íŠ¸ êµìœ¡í•™ì  ë¶„ì„

        Args:
            transcript: ì „ì²´ ë°œí™” í…ìŠ¤íŠ¸
            segments: STT ì„¸ê·¸ë¨¼íŠ¸ ëª©ë¡
            speaker_segments: í™”ì ë¶„ë¦¬ ê²°ê³¼

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not transcript or len(transcript) < 20:
            return self._empty_result()

        if self.use_llm:
            try:
                return self._analyze_llm(transcript, speaker_segments)
            except Exception as e:
                print(f"[DiscourseAnalyzer] LLM ë¶„ì„ ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ ì „í™˜: {e}")

        return self._analyze_rules(transcript, segments, speaker_segments)

    def _analyze_llm(self, transcript: str, speaker_segments: List[Dict] = None) -> Dict:
        """Gemini LLM ê¸°ë°˜ ë¶„ì„"""
        # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (í† í° ì ˆì•½)
        text = transcript[:3000] if len(transcript) > 3000 else transcript

        prompt = f"""ë‹¤ìŒì€ ì´ˆë“±í•™êµ ìˆ˜ì—…ì˜ êµì‚¬ ë°œí™” ë…¹ì·¨ë¡ì…ë‹ˆë‹¤. êµìœ¡í•™ì  ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”.

ë°œí™” í…ìŠ¤íŠ¸:
{text}

ë‹¤ìŒ í•­ëª©ì„ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:
1. question_types: ì§ˆë¬¸ ìœ í˜•ë³„ íšŸìˆ˜ (open_ended: ê°œë°©í˜•, closed: íì‡„í˜•, rhetorical: ìˆ˜ì‚¬ì , scaffolding: ìŠ¤ìºí´ë”©)
2. feedback_quality: í”¼ë“œë°± ìœ í˜•ë³„ íšŸìˆ˜ (specific_praise: êµ¬ì²´ì  ì¹­ì°¬, corrective: êµì •ì , generic: ì¼ë°˜ì )
3. bloom_levels: Bloom ì¸ì§€ìˆ˜ì¤€ ë¹„ìœ¨ (remember, understand, apply, analyze, evaluate, create) í•©ê³„ 1.0
4. interaction_score: ìƒí˜¸ì‘ìš© í’ˆì§ˆ ì ìˆ˜ (0-100)
5. key_observations: ì£¼ìš” ê´€ì°° ì‚¬í•­ (í•œêµ­ì–´, 3ê°œ)

JSONë§Œ ì¶œë ¥:"""

        model = genai.GenerativeModel("gemini-2.0-flash")
        response = model.generate_content(prompt)
        text_resp = response.text.strip()

        # JSON íŒŒì‹±
        import json
        # ```json ... ``` ë¸”ë¡ ì œê±°
        if "```" in text_resp:
            text_resp = re.sub(r'```(?:json)?\s*', '', text_resp)
            text_resp = text_resp.strip('` \n')

        result = json.loads(text_resp)

        # í•„ìˆ˜ í•„ë“œ ë³´ì¥
        return self._normalize_result(result)

    def _analyze_rules(self, transcript: str, segments: List[Dict] = None,
                       speaker_segments: List[Dict] = None) -> Dict:
        """ê·œì¹™ ê¸°ë°˜ íŒ¨í„´ ë§¤ì¹­ ë¶„ì„"""

        # 1. ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
        question_types = self._classify_questions_rules(transcript)

        # 2. í”¼ë“œë°± í’ˆì§ˆ ë¶„ì„
        feedback_quality = self._classify_feedback_rules(transcript)

        # 3. Bloom ì¸ì§€ìˆ˜ì¤€ ì¶”ì •
        bloom_levels = self._estimate_bloom_rules(transcript)

        # 4. ìƒí˜¸ì‘ìš© ì ìˆ˜
        interaction_score = self._calculate_interaction_score(
            question_types, feedback_quality, speaker_segments
        )

        return {
            "question_types": question_types,
            "feedback_quality": feedback_quality,
            "bloom_levels": bloom_levels,
            "interaction_score": round(interaction_score, 1),
            "key_observations": [],
            "analysis_method": "rules",
        }

    def _classify_questions_rules(self, text: str) -> Dict[str, int]:
        """ê·œì¹™ ê¸°ë°˜ ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜"""
        open_patterns = [
            r'ì™œ\s', r'ì–´ë–»ê²Œ\s', r'ì–´ë–¤\s', r'ë¬´ì—‡ì„', r'ë­˜\s',
            r'ì–´ë– í•œ', r'ì–´ì§¸ì„œ', r'ìƒê°[í•´í•˜]', r'ì˜ê²¬',
        ]
        closed_patterns = [
            r'ë§[ì§€ì£ ë‚˜]', r'ê·¸ë ‡[ì§€ì£ ]', r'[ì¸ì€ëŠ”ì´ê°€]\s*ê±°[ì£ ì§€]',
            r'\d+[ì´ê°€]\s*(ë§|ì•„ë‹Œ)', r'ì•Œ[ê² ì•˜]',
        ]
        scaffolding_patterns = [
            r'íŒíŠ¸', r'ë‹¤ì‹œ\s*í•œë²ˆ', r'ì°¨ê·¼ì°¨ê·¼', r'í•´ë³¼[ê¹Œë˜]',
            r'ê°™ì´\s*(í•´|í’€|ìƒê°)', r'ë„ì™€[ì¤„ì£¼]',
        ]
        rhetorical_patterns = [
            r'ê·¸ë ‡[ì§€ì£ ]\s*[?ï¼Ÿ]?$', r'ë‹¹ì—°[í•˜í•œ]', r'ë¬¼ë¡ ',
        ]

        open_count = sum(len(re.findall(p, text)) for p in open_patterns)
        closed_count = sum(len(re.findall(p, text)) for p in closed_patterns)
        scaffolding_count = sum(len(re.findall(p, text)) for p in scaffolding_patterns)
        rhetorical_count = sum(len(re.findall(p, text)) for p in rhetorical_patterns)

        return {
            "open_ended": open_count,
            "closed": closed_count,
            "scaffolding": scaffolding_count,
            "rhetorical": rhetorical_count,
        }

    def _classify_feedback_rules(self, text: str) -> Dict[str, int]:
        """ê·œì¹™ ê¸°ë°˜ í”¼ë“œë°± ìœ í˜• ë¶„ë¥˜"""
        specific_praise_patterns = [
            r'ì˜\s*í–ˆ', r'í›Œë¥­[í•´í•˜í•œ]', r'ë©‹[ì§€ì§„ì ¸]',
            r'ì •í™•[í•´í•˜í•œ]', r'ì¢‹[ì€ì•˜]', r'ëŒ€ë‹¨[í•´í•˜í•œ]',
        ]
        corrective_patterns = [
            r'ë‹¤ì‹œ\s*í•œë²ˆ', r'ì•„ë‹ˆ[ì•¼ìš”]', r'í‹€[ë¦°ë ¸]',
            r'ê³ [ì³ì¹˜]', r'ì£¼ì˜[í•´í•˜]', r'ì¡°ì‹¬',
        ]
        generic_patterns = [
            r'ê·¸ë˜[ìš”]?$', r'ë„¤[ì—]?$', r'ì‘[.?!]?$',
            r'ì¢‹ì•„[ìš”]?$', r'ì˜¤ì¼€ì´',
        ]

        return {
            "specific_praise": sum(len(re.findall(p, text)) for p in specific_praise_patterns),
            "corrective": sum(len(re.findall(p, text)) for p in corrective_patterns),
            "generic": sum(len(re.findall(p, text)) for p in generic_patterns),
        }

    def _estimate_bloom_rules(self, text: str) -> Dict[str, float]:
        """Bloom ì¸ì§€ìˆ˜ì¤€ ê·œì¹™ ê¸°ë°˜ ì¶”ì •"""
        remember_kw = ['ê¸°ì–µ', 'ì™¸ì›Œ', 'ì•”ê¸°', 'ë°˜ë³µ', 'ì½ì–´']
        understand_kw = ['ì„¤ëª…', 'ì´í•´', 'ì˜ë¯¸', 'ëœ»', 'ì™œëƒí•˜ë©´', 'ë•Œë¬¸']
        apply_kw = ['í™œìš©', 'ì ìš©', 'í’€ì–´', 'ê³„ì‚°', 'ì‚¬ìš©', 'í•´ë³´']
        analyze_kw = ['ë¹„êµ', 'ì°¨ì´', 'ë¶„ì„', 'ê´€ê³„', 'ì›ì¸', 'êµ¬ë¶„']
        evaluate_kw = ['í‰ê°€', 'íŒë‹¨', 'ì˜ê²¬', 'ìƒê°', 'ì¢‹ì€', 'ë‚˜ìœ']
        create_kw = ['ë§Œë“¤', 'ì„¤ê³„', 'ì°½ì‘', 'ë°œëª…', 'ìƒˆë¡œìš´', 'ìƒìƒ']

        counts = {
            "remember": sum(text.count(kw) for kw in remember_kw),
            "understand": sum(text.count(kw) for kw in understand_kw),
            "apply": sum(text.count(kw) for kw in apply_kw),
            "analyze": sum(text.count(kw) for kw in analyze_kw),
            "evaluate": sum(text.count(kw) for kw in evaluate_kw),
            "create": sum(text.count(kw) for kw in create_kw),
        }

        total = sum(counts.values()) or 1
        return {k: round(v / total, 2) for k, v in counts.items()}

    def _calculate_interaction_score(self, question_types: Dict,
                                     feedback_quality: Dict,
                                     speaker_segments: List[Dict] = None) -> float:
        """ìƒí˜¸ì‘ìš© í’ˆì§ˆ ì ìˆ˜ (0-100)"""
        score = 50.0  # ê¸°ë³¸ì 

        # ê°œë°©í˜• ì§ˆë¬¸ ë¹„ì¤‘
        total_q = sum(question_types.values()) or 1
        open_ratio = question_types.get("open_ended", 0) / total_q
        score += open_ratio * 20

        # ìŠ¤ìºí´ë”© ì§ˆë¬¸ ë³´ë„ˆìŠ¤
        score += min(10, question_types.get("scaffolding", 0) * 3)

        # êµ¬ì²´ì  ì¹­ì°¬
        score += min(10, feedback_quality.get("specific_praise", 0) * 2)

        # êµì • í”¼ë“œë°±
        score += min(5, feedback_quality.get("corrective", 0) * 1.5)

        # í™”ì ë¶„ë¦¬ ë°ì´í„° í™œìš©
        if speaker_segments:
            student_turns = sum(1 for s in speaker_segments if s.get("speaker") == "student")
            score += min(10, student_turns * 1.5)

        return min(100, max(0, score))

    def _normalize_result(self, result: Dict) -> Dict:
        """LLM ê²°ê³¼ ì •ê·œí™”"""
        defaults = self._empty_result()
        for key in defaults:
            if key not in result:
                result[key] = defaults[key]
        result["analysis_method"] = "llm"
        return result

    def _empty_result(self) -> Dict:
        """ë¹ˆ ê²°ê³¼"""
        return {
            "question_types": {"open_ended": 0, "closed": 0, "scaffolding": 0, "rhetorical": 0},
            "feedback_quality": {"specific_praise": 0, "corrective": 0, "generic": 0},
            "bloom_levels": {"remember": 0.3, "understand": 0.4, "apply": 0.2, "analyze": 0.1, "evaluate": 0, "create": 0},
            "interaction_score": 50.0,
            "key_observations": [],
            "analysis_method": "none",
        }
