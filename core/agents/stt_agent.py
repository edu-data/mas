"""
ğŸ—£ï¸ STT Agent - ìŒì„±â†’í…ìŠ¤íŠ¸ ë³€í™˜ ì „ë¬¸ ì—ì´ì „íŠ¸
v5.0: í™”ì ë¶„ë¦¬(Speaker Diarization) + ìƒí˜¸ì‘ìš© ë¶„ì„ ì¶”ê°€
"""

import re
import subprocess
import numpy as np
from pathlib import Path
from typing import Dict, Optional, List
from dataclasses import dataclass, field


# ì„ íƒì  ì˜ì¡´ì„±
try:
    import whisper
    HAS_WHISPER = True
except ImportError:
    HAS_WHISPER = False

try:
    from faster_whisper import WhisperModel
    HAS_FASTER_WHISPER = True
except ImportError:
    HAS_FASTER_WHISPER = False

try:
    import librosa
    HAS_LIBROSA = True
except ImportError:
    HAS_LIBROSA = False


# í•œêµ­ì–´ êµìœ¡í•™ í•„ëŸ¬ ë‹¨ì–´
KOREAN_FILLER_WORDS = [
    "ìŒ", "ì–´", "ì´ì œ", "ê·¸ë˜ì„œ", "ì", "ë„¤", "ì˜ˆ",
    "ê·¸ëŸ¬ë‹ˆê¹Œ", "ë­", "ì•½ê°„", "ì¢€", "í•œë²ˆ", "ê·¸ëƒ¥"
]


@dataclass
class SpeakerSegment:
    """í™”ìë³„ ë°œí™” êµ¬ê°„"""
    start: float
    end: float
    speaker: str           # "teacher" or "student"
    text: str = ""
    energy: float = 0.0


@dataclass
class STTResult:
    """STT ë¶„ì„ ê²°ê³¼"""
    transcript: str = ""
    word_count: int = 0
    speaking_rate: float = 0.0        # WPM (Words Per Minute)
    duration_seconds: float = 0.0
    filler_words: Dict[str, int] = field(default_factory=dict)
    filler_ratio: float = 0.0
    language: str = "ko"
    segments: List[Dict] = field(default_factory=list)
    confidence: float = 0.0
    method: str = "fallback"           # whisper / faster_whisper / fallback
    # v5.0 í™”ì ë¶„ë¦¬ í•„ë“œ
    speaker_segments: List[Dict] = field(default_factory=list)
    teacher_ratio: float = 0.75
    student_turns: int = 0
    interaction_count: int = 0          # êµì‚¬â†”í•™ìƒ êµëŒ€ íšŸìˆ˜
    question_count: int = 0             # ì§ˆë¬¸ íšŸìˆ˜ (ë¬¸ì¥ë¶€í˜¸ ê¸°ë°˜)

    def to_dict(self) -> Dict:
        return {
            "transcript": self.transcript[:500] if self.transcript else "",
            "transcript_length": len(self.transcript),
            "word_count": self.word_count,
            "speaking_rate": round(self.speaking_rate, 1),
            "speaking_pattern": self._classify_speaking_pattern(),
            "duration_seconds": round(self.duration_seconds, 1),
            "filler_words": self.filler_words,
            "filler_ratio": round(self.filler_ratio, 3),
            "filler_count": sum(self.filler_words.values()),
            "language": self.language,
            "segment_count": len(self.segments),
            "confidence": round(self.confidence, 2),
            "method": self.method,
            # v5.0
            "teacher_ratio": round(self.teacher_ratio, 3),
            "student_turns": self.student_turns,
            "interaction_count": self.interaction_count,
            "question_count": self.question_count,
            "speaker_segment_count": len(self.speaker_segments),
        }

    def _classify_speaking_pattern(self) -> str:
        """ë§í•˜ê¸° íŒ¨í„´ ë¶„ë¥˜"""
        if self.speaking_rate < 80:
            return "ëŠë¦¼ (Slow)"
        elif self.speaking_rate < 120:
            return "ëŒ€í™”í˜• (Conversational)"
        elif self.speaking_rate < 160:
            return "ê°•ì˜í˜• (Lecture)"
        else:
            return "ë¹ ë¦„ (Fast)"


class STTAgent:
    """
    ğŸ—£ï¸ STT Agent v5.0
    ìŒì„± ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ì–¸ì–´ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.

    v5.0 ì¶”ê°€:
    - ì—ë„ˆì§€/ë°œí™”ê¸¸ì´ ê¸°ë°˜ í™”ì ë¶„ë¦¬ (ê²½ëŸ‰)
    - ì§ˆë¬¸ íšŸìˆ˜ ê°ì§€
    - êµì‚¬-í•™ìƒ êµëŒ€ íšŸìˆ˜ ì¸¡ì •
    """

    def __init__(self, model_size: str = "base", language: str = "ko"):
        self.model_size = model_size
        self.language = language
        self._whisper_model = None
        self._faster_model = None

    def analyze(self, audio_path: str) -> Dict:
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„ì„

        Args:
            audio_path: WAV/MP3 ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ

        Returns:
            STT ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        path = Path(audio_path)
        if not path.exists():
            return STTResult(method="error").to_dict()

        duration = self._get_audio_duration(str(path))

        # ì—”ì§„ ìš°ì„ ìˆœìœ„: Faster-Whisper > Whisper > Fallback
        if HAS_FASTER_WHISPER:
            result = self._analyze_faster_whisper(str(path), duration)
        elif HAS_WHISPER:
            result = self._analyze_whisper(str(path), duration)
        else:
            result = self._analyze_fallback(str(path), duration)

        # v5.0: í™”ì ë¶„ë¦¬ + ì§ˆë¬¸ ê°ì§€
        if result.segments and HAS_LIBROSA:
            self._simple_diarization(result, str(path))
        self._detect_questions(result)

        return result.to_dict()

    def analyze_from_video(self, video_path: str) -> Dict:
        """ë¹„ë””ì˜¤ì—ì„œ ì§ì ‘ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¶„ì„"""
        import tempfile
        import os

        temp_audio = os.path.join(tempfile.gettempdir(), "gaim_stt_temp.wav")

        try:
            subprocess.run([
                "ffmpeg", "-y", "-i", video_path,
                "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1",
                temp_audio
            ], capture_output=True, timeout=120)

            if Path(temp_audio).exists():
                return self.analyze(temp_audio)
        except Exception:
            pass

        # FFprobe í´ë°±
        duration = self._get_video_duration(video_path)
        return self._analyze_fallback(video_path, duration).to_dict()

    def _analyze_faster_whisper(self, audio_path: str, duration: float) -> STTResult:
        """Faster-Whisper ì—”ì§„ìœ¼ë¡œ STT ìˆ˜í–‰"""
        try:
            if self._faster_model is None:
                self._faster_model = WhisperModel(
                    self.model_size, device="cpu", compute_type="int8"
                )

            segments, info = self._faster_model.transcribe(
                audio_path, language=self.language, beam_size=5
            )

            all_text = []
            seg_list = []
            for seg in segments:
                all_text.append(seg.text)
                seg_list.append({
                    "start": round(seg.start, 2),
                    "end": round(seg.end, 2),
                    "text": seg.text.strip(),
                })

            transcript = " ".join(all_text)
            word_count = len(transcript.split())

            result = STTResult(
                transcript=transcript,
                word_count=word_count,
                speaking_rate=round(word_count / (duration / 60), 1) if duration > 0 else 0,
                duration_seconds=duration,
                language=self.language,
                segments=seg_list,
                confidence=0.85,
                method="faster_whisper",
            )
            self._detect_fillers(result)
            return result

        except Exception as e:
            print(f"[STT Agent] Faster-Whisper ì˜¤ë¥˜: {e}")
            return self._analyze_fallback(audio_path, duration)

    def _analyze_whisper(self, audio_path: str, duration: float) -> STTResult:
        """OpenAI Whisper ì—”ì§„ìœ¼ë¡œ STT ìˆ˜í–‰"""
        try:
            if self._whisper_model is None:
                self._whisper_model = whisper.load_model(self.model_size)

            result = self._whisper_model.transcribe(
                audio_path, language=self.language
            )

            transcript = result.get("text", "")
            word_count = len(transcript.split())
            segments = [
                {
                    "start": round(s["start"], 2),
                    "end": round(s["end"], 2),
                    "text": s["text"].strip(),
                }
                for s in result.get("segments", [])
            ]

            stt_result = STTResult(
                transcript=transcript,
                word_count=word_count,
                speaking_rate=round(word_count / (duration / 60), 1) if duration > 0 else 0,
                duration_seconds=duration,
                language=self.language,
                segments=segments,
                confidence=0.80,
                method="whisper",
            )
            self._detect_fillers(stt_result)
            return stt_result

        except Exception as e:
            print(f"[STT Agent] Whisper ì˜¤ë¥˜: {e}")
            return self._analyze_fallback(audio_path, duration)

    def _analyze_fallback(self, file_path: str, duration: float) -> STTResult:
        """ML ì—†ì´ FFprobe ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¶”ì • ë¶„ì„"""
        if duration <= 0:
            duration = self._get_audio_duration(file_path) or 600.0

        estimated_wpm = 125.0
        speaking_ratio = 0.75
        estimated_word_count = int(estimated_wpm * (duration / 60) * speaking_ratio)

        return STTResult(
            transcript="[í´ë°± ëª¨ë“œ: ì‹¤ì œ í…ìŠ¤íŠ¸ ë³€í™˜ ì—†ìŒ. Whisper ì„¤ì¹˜ í•„ìš”]",
            word_count=estimated_word_count,
            speaking_rate=estimated_wpm,
            duration_seconds=duration,
            filler_words={},
            filler_ratio=0.03,
            language=self.language,
            confidence=0.40,
            method="fallback",
        )

    # ================================================================
    # v5.0: í™”ì ë¶„ë¦¬ (Speaker Diarization)
    # ================================================================

    def _simple_diarization(self, result: STTResult, audio_path: str):
        """
        ì—ë„ˆì§€/ë°œí™” ê¸¸ì´ ê¸°ë°˜ ê²½ëŸ‰ í™”ì ë¶„ë¦¬

        ì›ë¦¬: êµì‚¬ëŠ” ì£¼ë¡œ ê¸´ ë°œí™”(ì„¤ëª…), í•™ìƒì€ ì§§ì€ ì‘ë‹µ
        - ê¸´ ì„¸ê·¸ë¨¼íŠ¸ (>3ì´ˆ) â†’ êµì‚¬ ë°œí™”
        - ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ (<2ì´ˆ) + ì• ì„¸ê·¸ë¨¼íŠ¸ì™€ ì—ë„ˆì§€ ì°¨ì´ â†’ í•™ìƒ ë°œí™”
        """
        try:
            y, sr = librosa.load(audio_path, sr=16000)
        except Exception:
            return

        speaker_segs = []
        prev_speaker = "teacher"
        interaction_count = 0

        for seg in result.segments:
            start = seg.get("start", 0)
            end = seg.get("end", 0)
            text = seg.get("text", "")
            seg_duration = end - start

            # ì—ë„ˆì§€ ê³„ì‚°
            start_sample = int(start * sr)
            end_sample = min(int(end * sr), len(y))
            if end_sample <= start_sample:
                continue
            segment_audio = y[start_sample:end_sample]
            energy = float(np.sqrt(np.mean(segment_audio ** 2)))

            # í™”ì íŒë³„ ê·œì¹™
            if seg_duration < 1.5 and len(text.split()) < 5:
                # ë§¤ìš° ì§§ì€ ì‘ë‹µ â†’ í•™ìƒ (ë†’ì€ í™•ë¥ )
                speaker = "student"
            elif seg_duration < 2.5 and self._is_response_pattern(text):
                # ì§§ì€ ì‘ë‹µ íŒ¨í„´ â†’ í•™ìƒ
                speaker = "student"
            else:
                speaker = "teacher"

            # êµëŒ€ íšŸìˆ˜ ì¹´ìš´íŠ¸
            if speaker != prev_speaker:
                interaction_count += 1
            prev_speaker = speaker

            speaker_segs.append({
                "start": start,
                "end": end,
                "speaker": speaker,
                "text": text,
                "energy": round(energy, 4),
            })

        result.speaker_segments = speaker_segs
        result.interaction_count = interaction_count

        # êµì‚¬/í•™ìƒ ë°œí™” ë¹„ìœ¨
        teacher_time = sum(
            s["end"] - s["start"] for s in speaker_segs if s["speaker"] == "teacher"
        )
        student_time = sum(
            s["end"] - s["start"] for s in speaker_segs if s["speaker"] == "student"
        )
        total_time = teacher_time + student_time
        result.teacher_ratio = teacher_time / total_time if total_time > 0 else 0.75
        result.student_turns = sum(1 for s in speaker_segs if s["speaker"] == "student")

    def _is_response_pattern(self, text: str) -> bool:
        """í•™ìƒ ì‘ë‹µ íŒ¨í„´ ê°ì§€"""
        response_patterns = [
            "ë„¤", "ì˜ˆ", "ì•„", "ë§ì•„ìš”", "ì•Œê² ìŠµë‹ˆë‹¤", "ê°ì‚¬í•©ë‹ˆë‹¤",
            "ì„ ìƒë‹˜", "ì €ìš”", "ì—¬ê¸°ìš”", "ë‹¤ì„¯", "í•˜ë‚˜", "ë‘˜", "ì…‹", "ë„·",
        ]
        text_stripped = text.strip()
        for pat in response_patterns:
            if text_stripped == pat or text_stripped.startswith(pat + " "):
                return True
        return len(text_stripped) < 10

    # ================================================================
    # v5.0: ì§ˆë¬¸ ê°ì§€
    # ================================================================

    def _detect_questions(self, result: STTResult):
        """ë°œí™”ì—ì„œ ì§ˆë¬¸ íšŸìˆ˜ ê°ì§€"""
        if not result.transcript:
            return

        # ë¬¼ìŒí‘œ ê¸°ë°˜
        q_mark_count = result.transcript.count("?")

        # í•œêµ­ì–´ ì§ˆë¬¸ íŒ¨í„´ (? ì—†ëŠ” ê²½ìš°ë„ ê°ì§€)
        question_patterns = [
            r'[ê°€-í£]+\s*(?:í• ê¹Œìš”|í• ë˜ìš”|í• ê²Œìš”|í•´ë³¼ê¹Œ)',
            r'[ê°€-í£]+\s*(?:ì¼ê¹Œìš”|ì¸ê°€ìš”|ë‚˜ìš”|ê¹Œìš”|ã„¹ê¹Œ)',
            r'ë­ê°€|ì–´ë–¤|ì™œ|ì–´ë–»ê²Œ|ëª‡\s*(?:ê°œ|ëª…|ë²ˆ)',
            r'ì•Œê² [ì§€ì£ ]|ì´í•´[í–ˆí•˜]|ë§[ì§€ì£ ë‚˜]',
        ]

        pattern_count = 0
        for pat in question_patterns:
            pattern_count += len(re.findall(pat, result.transcript))

        result.question_count = max(q_mark_count, pattern_count)

    # ================================================================
    # ê¸°ì¡´ ìœ í‹¸ë¦¬í‹°
    # ================================================================

    def _detect_fillers(self, result: STTResult):
        """í•œêµ­ì–´ í•„ëŸ¬ ë‹¨ì–´ ê°ì§€"""
        if not result.transcript:
            return

        text = result.transcript
        filler_counts = {}
        total_fillers = 0

        for filler in KOREAN_FILLER_WORDS:
            count = len(re.findall(rf'\b{re.escape(filler)}\b', text))
            if count > 0:
                filler_counts[filler] = count
                total_fillers += count

        result.filler_words = filler_counts
        if result.word_count > 0:
            result.filler_ratio = total_fillers / result.word_count

    def _get_audio_duration(self, path: str) -> float:
        """FFprobeë¡œ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¡°íšŒ"""
        try:
            cmd = [
                "ffprobe", "-v", "quiet", "-show_entries",
                "format=duration", "-of", "csv=p=0", path
            ]
            out = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
            return float(out.stdout.strip())
        except Exception:
            return 0.0

    def _get_video_duration(self, path: str) -> float:
        """FFprobeë¡œ ë¹„ë””ì˜¤ ê¸¸ì´ ì¡°íšŒ"""
        return self._get_audio_duration(path)
