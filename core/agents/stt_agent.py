"""
ğŸ—£ï¸ STT Agent - ìŒì„±â†’í…ìŠ¤íŠ¸ ë³€í™˜ ì „ë¬¸ ì—ì´ì „íŠ¸
v6.1: pyannote.audio ì‹¤ì œ í™”ì ë¶„ë¦¬ + íœ´ë¦¬ìŠ¤í‹± í´ë°±
"""

import re
import os
import subprocess
import numpy as np
from pathlib import Path
from typing import Dict, Optional, List
from dataclasses import dataclass, field


# ì„ íƒì  ì˜ì¡´ì„±
try:
    import whisper
    HAS_WHISPER = True
except ImportError:
    HAS_WHISPER = False

try:
    from faster_whisper import WhisperModel
    HAS_FASTER_WHISPER = True
except ImportError:
    HAS_FASTER_WHISPER = False

try:
    import librosa
    HAS_LIBROSA = True
except ImportError:
    HAS_LIBROSA = False

try:
    # == í˜¸í™˜ì„± íŒ¨ì¹˜: pyannote 4.0.xì˜ ë‚´ë¶€ ì˜ì¡´ì„±ì´ ì‚¬ìš©í•˜ëŠ” ì œê±°ëœ API ë³µì› ==
    # torchaudio 2.2+ ì—ì„œ ì œê±°ëœ backend API ìŠ¤í…
    import torchaudio, sys as _sys, types
    if not hasattr(torchaudio, 'set_audio_backend'):
        torchaudio.set_audio_backend = lambda x: None
    if not hasattr(torchaudio, 'list_audio_backends'):
        torchaudio.list_audio_backends = lambda: ['soundfile']
    if not hasattr(torchaudio, 'get_audio_backend'):
        torchaudio.get_audio_backend = lambda: 'soundfile'
    # torchaudio.backend.common.AudioMetaData ëª¨ë“ˆ mock (pyannote ë‚´ë¶€ ì˜ì¡´)
    if 'torchaudio.backend' not in _sys.modules:
        from collections import namedtuple
        _AudioMetaData = namedtuple('AudioMetaData', [
            'sample_rate', 'num_frames', 'num_channels', 'bits_per_sample', 'encoding'
        ], defaults=[0, 0, 0, 0, ''])
        _backend = types.ModuleType('torchaudio.backend')
        _backend_common = types.ModuleType('torchaudio.backend.common')
        _backend_common.AudioMetaData = _AudioMetaData
        _backend.common = _backend_common
        _sys.modules['torchaudio.backend'] = _backend
        _sys.modules['torchaudio.backend.common'] = _backend_common
    # torchaudio.info ìŠ¤í… (2.10ì—ì„œ ì œê±°ë¨, speechbrain ë‚´ë¶€ ì˜ì¡´)
    if not hasattr(torchaudio, 'info'):
        def _torchaudio_info(path, **kwargs):
            import soundfile as _sf
            _i = _sf.info(str(path))
            from collections import namedtuple as _nt
            _AM = _nt('AudioMetaData', ['sample_rate','num_frames','num_channels','bits_per_sample','encoding'],
                       defaults=[0, 0, 0, 0, ''])
            return _AM(sample_rate=_i.samplerate, num_frames=_i.frames,
                       num_channels=_i.channels, bits_per_sample=_i.subtype_info.split()[0] if _i.subtype_info else 16,
                       encoding=_i.subtype or '')
        torchaudio.info = _torchaudio_info
    # numpy 2.0+ì—ì„œ ì œê±°ëœ np.NaN, np.NAN ë³µì›
    import numpy as _np
    if not hasattr(_np, 'NaN'):
        _np.NaN = _np.nan
    if not hasattr(_np, 'NAN'):
        _np.NAN = _np.nan
    # torchcodec DLL ë¡œë”© ì‹¤íŒ¨ ì‹œ mock ëª¨ë“ˆ ì£¼ì… (Windows í˜¸í™˜)
    try:
        from torchcodec.decoders import AudioDecoder
    except Exception:
        import types, sys as _sys
        _tc = types.ModuleType('torchcodec')
        _tc_dec = types.ModuleType('torchcodec.decoders')
        class _MockAudioDecoder:
            def __init__(self, src): self._src = src
            @property
            def metadata(self):
                import soundfile as _sf
                info = _sf.info(self._src)
                class _M:
                    sample_rate = info.samplerate
                    num_channels = info.channels
                    num_frames = info.frames
                    duration_seconds_from_header = info.frames / info.samplerate
                return _M()
            def get_all_samples(self):
                import soundfile as _sf, torch
                data, sr = _sf.read(self._src, dtype='float32')
                if data.ndim == 1: data = data[None, :]
                else: data = data.T
                class _S:
                    pass
                s = _S(); s.data = torch.from_numpy(data); s.sample_rate = sr
                return s
            def get_samples_played_in_range(self, start, end):
                import soundfile as _sf, torch
                info = _sf.info(self._src)
                sr = info.samplerate
                start_f = int(start * sr); end_f = int(end * sr)
                data, _ = _sf.read(self._src, start=start_f, stop=end_f, dtype='float32')
                if data.ndim == 1: data = data[None, :]
                else: data = data.T
                class _S:
                    pass
                s = _S(); s.data = torch.from_numpy(data); s.sample_rate = sr
                return s
        class _MockAudioStreamMetadata:
            pass
        class _MockAudioSamples:
            pass
        _tc_dec.AudioDecoder = _MockAudioDecoder
        _tc_dec.AudioStreamMetadata = _MockAudioStreamMetadata
        _tc.AudioSamples = _MockAudioSamples
        _tc.decoders = _tc_dec
        _sys.modules['torchcodec'] = _tc
        _sys.modules['torchcodec.decoders'] = _tc_dec

    # huggingface_hub 1.x: use_auth_token â†’ token ì¼ê´„ ë³€í™˜
    import huggingface_hub as _hfh
    import functools
    def _hf_compat_wrapper(orig_fn):
        @functools.wraps(orig_fn)
        def wrapper(*args, **kwargs):
            if 'use_auth_token' in kwargs:
                kwargs['token'] = kwargs.pop('use_auth_token')
            return orig_fn(*args, **kwargs)
        return wrapper
    for _fn_name in ['hf_hub_download', 'model_info', 'upload_file', 'create_repo']:
        if hasattr(_hfh, _fn_name):
            setattr(_hfh, _fn_name, _hf_compat_wrapper(getattr(_hfh, _fn_name)))
    # HfApi ë©”ì„œë“œë„ íŒ¨ì¹˜
    if hasattr(_hfh, 'HfApi'):
        for _m in ['model_info', 'hf_hub_download']:
            if hasattr(_hfh.HfApi, _m):
                setattr(_hfh.HfApi, _m, _hf_compat_wrapper(getattr(_hfh.HfApi, _m)))
    # torch.load í˜¸í™˜: pyannote ëª¨ë¸ì´ weights_only=True (PyTorch 2.10 ê¸°ë³¸ê°’)ì—ì„œ ì‹¤íŒ¨
    # torch.serialization.load ë ˆë²¨ì—ì„œ íŒ¨ì¹˜í•´ì•¼ pyannote ë‚´ë¶€ í˜¸ì¶œë„ ì ìš©ë¨
    import torch, torch.serialization
    _original_torch_ser_load = torch.serialization.load
    @functools.wraps(_original_torch_ser_load)
    def _compat_torch_load(*args, **kwargs):
        kwargs['weights_only'] = False
        return _original_torch_ser_load(*args, **kwargs)
    torch.serialization.load = _compat_torch_load
    torch.load = _compat_torch_load

    from pyannote.audio import Pipeline as PyannotePipeline
    # pyannote ë‚´ë¶€ ëª¨ë“ˆì˜ ë¡œì»¬ ì°¸ì¡° íŒ¨ì¹˜
    import pyannote.audio.core.pipeline as _pa_pipeline
    import pyannote.audio.core.model as _pa_model
    if hasattr(_pa_pipeline, 'hf_hub_download'):
        _pa_pipeline.hf_hub_download = _hf_compat_wrapper(_pa_pipeline.hf_hub_download)
    if hasattr(_pa_model, 'hf_hub_download'):
        _pa_model.hf_hub_download = _hf_compat_wrapper(_pa_model.hf_hub_download)
    # pyannote model ëª¨ë“ˆì˜ torch.loadë„ íŒ¨ì¹˜
    _pa_model.torch.load = _compat_torch_load
    _pa_model.torch.serialization.load = _compat_torch_load
    HAS_PYANNOTE = True
except (ImportError, Exception) as _pyannote_err:
    HAS_PYANNOTE = False


# í•œêµ­ì–´ êµìœ¡í•™ í•„ëŸ¬ ë‹¨ì–´
KOREAN_FILLER_WORDS = [
    "ìŒ", "ì–´", "ì´ì œ", "ê·¸ë˜ì„œ", "ì", "ë„¤", "ì˜ˆ",
    "ê·¸ëŸ¬ë‹ˆê¹Œ", "ë­", "ì•½ê°„", "ì¢€", "í•œë²ˆ", "ê·¸ëƒ¥"
]


@dataclass
class SpeakerSegment:
    """í™”ìë³„ ë°œí™” êµ¬ê°„"""
    start: float
    end: float
    speaker: str           # "teacher" or "student"
    text: str = ""
    energy: float = 0.0


@dataclass
class STTResult:
    """STT ë¶„ì„ ê²°ê³¼"""
    transcript: str = ""
    word_count: int = 0
    speaking_rate: float = 0.0        # WPM (Words Per Minute)
    duration_seconds: float = 0.0
    filler_words: Dict[str, int] = field(default_factory=dict)
    filler_ratio: float = 0.0
    language: str = "ko"
    segments: List[Dict] = field(default_factory=list)
    confidence: float = 0.0
    method: str = "fallback"           # whisper / faster_whisper / fallback
    # v5.0 í™”ì ë¶„ë¦¬ í•„ë“œ
    speaker_segments: List[Dict] = field(default_factory=list)
    teacher_ratio: float = 0.75
    student_turns: int = 0
    interaction_count: int = 0          # êµì‚¬â†”í•™ìƒ êµëŒ€ íšŸìˆ˜
    question_count: int = 0             # ì§ˆë¬¸ íšŸìˆ˜ (ë¬¸ì¥ë¶€í˜¸ ê¸°ë°˜)
    diarization_method: str = "none"    # v6.1: "pyannote" / "heuristic" / "none"

    def to_dict(self) -> Dict:
        return {
            "transcript": self.transcript[:500] if self.transcript else "",
            "transcript_length": len(self.transcript),
            "word_count": self.word_count,
            "speaking_rate": round(self.speaking_rate, 1),
            "speaking_pattern": self._classify_speaking_pattern(),
            "duration_seconds": round(self.duration_seconds, 1),
            "filler_words": self.filler_words,
            "filler_ratio": round(self.filler_ratio, 3),
            "filler_count": sum(self.filler_words.values()),
            "language": self.language,
            "segment_count": len(self.segments),
            "confidence": round(self.confidence, 2),
            "method": self.method,
            # v5.0
            "teacher_ratio": round(self.teacher_ratio, 3),
            "student_turns": self.student_turns,
            "interaction_count": self.interaction_count,
            "question_count": self.question_count,
            "speaker_segment_count": len(self.speaker_segments),
            "diarization_method": self.diarization_method,
        }

    def _classify_speaking_pattern(self) -> str:
        """ë§í•˜ê¸° íŒ¨í„´ ë¶„ë¥˜"""
        if self.speaking_rate < 80:
            return "ëŠë¦¼ (Slow)"
        elif self.speaking_rate < 120:
            return "ëŒ€í™”í˜• (Conversational)"
        elif self.speaking_rate < 160:
            return "ê°•ì˜í˜• (Lecture)"
        else:
            return "ë¹ ë¦„ (Fast)"


class STTAgent:
    """
    ğŸ—£ï¸ STT Agent v6.1
    ìŒì„± ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ì–¸ì–´ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.

    v6.1 ì¶”ê°€:
    - pyannote.audio ê¸°ë°˜ ì‹¤ì œ í™”ì ë¶„ë¦¬ (DNN)
    - Whisper ì„¸ê·¸ë¨¼íŠ¸ â†” pyannote íƒ€ì„ë¼ì¸ IOU ë§¤ì¹­
    - í´ë°±: pyannote ì‹¤íŒ¨ ì‹œ ì—ë„ˆì§€/ë°œí™” íœ´ë¦¬ìŠ¤í‹±
    """

    def __init__(self, model_size: str = "base", language: str = "ko"):
        self.model_size = model_size
        self.language = language
        self._whisper_model = None
        self._faster_model = None
        self._pyannote_pipeline = None

    def analyze(self, audio_path: str) -> Dict:
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„ì„

        Args:
            audio_path: WAV/MP3 ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ

        Returns:
            STT ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        path = Path(audio_path)
        if not path.exists():
            return STTResult(method="error").to_dict()

        duration = self._get_audio_duration(str(path))

        # ì—”ì§„ ìš°ì„ ìˆœìœ„: Faster-Whisper > Whisper > Fallback
        if HAS_FASTER_WHISPER:
            result = self._analyze_faster_whisper(str(path), duration)
        elif HAS_WHISPER:
            result = self._analyze_whisper(str(path), duration)
        else:
            result = self._analyze_fallback(str(path), duration)

        # v6.1: í™”ì ë¶„ë¦¬ â€” pyannote ìš°ì„ , í´ë°± heuristic
        diarization_done = False
        if result.segments and HAS_PYANNOTE:
            try:
                self._pyannote_diarization(result, str(path))
                diarization_done = True
            except Exception as e:
                print(f"[STT Agent] pyannote í™”ì ë¶„ë¦¬ ì‹¤íŒ¨, íœ´ë¦¬ìŠ¤í‹± í´ë°±: {e}")

        if not diarization_done and result.segments and HAS_LIBROSA:
            self._simple_diarization(result, str(path))

        self._detect_questions(result)

        return result.to_dict()

    def analyze_from_video(self, video_path: str) -> Dict:
        """ë¹„ë””ì˜¤ì—ì„œ ì§ì ‘ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¶„ì„"""
        import tempfile
        import os

        temp_audio = os.path.join(tempfile.gettempdir(), "gaim_stt_temp.wav")

        try:
            subprocess.run([
                "ffmpeg", "-y", "-i", video_path,
                "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1",
                temp_audio
            ], capture_output=True, timeout=120)

            if Path(temp_audio).exists():
                return self.analyze(temp_audio)
        except Exception:
            pass

        # FFprobe í´ë°±
        duration = self._get_video_duration(video_path)
        return self._analyze_fallback(video_path, duration).to_dict()

    def _analyze_faster_whisper(self, audio_path: str, duration: float) -> STTResult:
        """Faster-Whisper ì—”ì§„ìœ¼ë¡œ STT ìˆ˜í–‰"""
        try:
            if self._faster_model is None:
                self._faster_model = WhisperModel(
                    self.model_size, device="cpu", compute_type="int8"
                )

            segments, info = self._faster_model.transcribe(
                audio_path, language=self.language, beam_size=5
            )

            all_text = []
            seg_list = []
            for seg in segments:
                all_text.append(seg.text)
                seg_list.append({
                    "start": round(seg.start, 2),
                    "end": round(seg.end, 2),
                    "text": seg.text.strip(),
                })

            transcript = " ".join(all_text)
            word_count = len(transcript.split())

            result = STTResult(
                transcript=transcript,
                word_count=word_count,
                speaking_rate=round(word_count / (duration / 60), 1) if duration > 0 else 0,
                duration_seconds=duration,
                language=self.language,
                segments=seg_list,
                confidence=0.85,
                method="faster_whisper",
            )
            self._detect_fillers(result)
            return result

        except Exception as e:
            print(f"[STT Agent] Faster-Whisper ì˜¤ë¥˜: {e}")
            return self._analyze_fallback(audio_path, duration)

    def _analyze_whisper(self, audio_path: str, duration: float) -> STTResult:
        """OpenAI Whisper ì—”ì§„ìœ¼ë¡œ STT ìˆ˜í–‰"""
        try:
            if self._whisper_model is None:
                self._whisper_model = whisper.load_model(self.model_size)

            result = self._whisper_model.transcribe(
                audio_path, language=self.language
            )

            transcript = result.get("text", "")
            word_count = len(transcript.split())
            segments = [
                {
                    "start": round(s["start"], 2),
                    "end": round(s["end"], 2),
                    "text": s["text"].strip(),
                }
                for s in result.get("segments", [])
            ]

            stt_result = STTResult(
                transcript=transcript,
                word_count=word_count,
                speaking_rate=round(word_count / (duration / 60), 1) if duration > 0 else 0,
                duration_seconds=duration,
                language=self.language,
                segments=segments,
                confidence=0.80,
                method="whisper",
            )
            self._detect_fillers(stt_result)
            return stt_result

        except Exception as e:
            print(f"[STT Agent] Whisper ì˜¤ë¥˜: {e}")
            return self._analyze_fallback(audio_path, duration)

    def _analyze_fallback(self, file_path: str, duration: float) -> STTResult:
        """ML ì—†ì´ FFprobe ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¶”ì • ë¶„ì„"""
        if duration <= 0:
            duration = self._get_audio_duration(file_path) or 600.0

        estimated_wpm = 125.0
        speaking_ratio = 0.75
        estimated_word_count = int(estimated_wpm * (duration / 60) * speaking_ratio)

        return STTResult(
            transcript="[í´ë°± ëª¨ë“œ: ì‹¤ì œ í…ìŠ¤íŠ¸ ë³€í™˜ ì—†ìŒ. Whisper ì„¤ì¹˜ í•„ìš”]",
            word_count=estimated_word_count,
            speaking_rate=estimated_wpm,
            duration_seconds=duration,
            filler_words={},
            filler_ratio=0.03,
            language=self.language,
            confidence=0.40,
            method="fallback",
        )

    # ================================================================
    # v6.1: pyannote.audio ì‹¤ì œ í™”ì ë¶„ë¦¬
    # ================================================================

    def _pyannote_diarization(self, result: STTResult, audio_path: str):
        """
        pyannote.audio ê¸°ë°˜ ì‹¤ì œ í™”ì ë¶„ë¦¬ (v6.1)

        Whisper ì„¸ê·¸ë¨¼íŠ¸ì™€ pyannote íƒ€ì„ë¼ì¸ì„ IOU ë§¤ì¹­í•˜ì—¬
        ê° ë°œí™”ì— í™”ìë¥¼ í• ë‹¹í•œë‹¤.
        êµì‚¬ = ê°€ì¥ ê¸´ ë°œí™” ì‹œê°„ì˜ í™”ì (ìˆ˜ì—… ì‹¤ì—°ì—ì„œ êµì‚¬ê°€ ëŒ€ë¶€ë¶„ ë°œí™”)
        """
        import torch

        hf_token = os.getenv("HF_TOKEN", "")
        if not hf_token:
            raise ValueError("HF_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # íŒŒì´í”„ë¼ì¸ ë¡œë“œ (ìºì‹±)
        if self._pyannote_pipeline is None:
            os.environ["HUGGING_FACE_HUB_TOKEN"] = hf_token
            os.environ["HF_TOKEN"] = hf_token
            self._pyannote_pipeline = PyannotePipeline.from_pretrained(
                "pyannote/speaker-diarization-3.1",
            )

        # í™”ì ë¶„ë¦¬ ì‹¤í–‰ (í™”ì ìˆ˜ íŒíŠ¸: êµì‚¬ + í•™ìƒë“¤)
        # torchcodec mockì´ soundfileë¡œ ì˜¤ë””ì˜¤ë¥¼ ì½ì–´ pyannoteì— ì „ë‹¬
        diarization = self._pyannote_pipeline(audio_path, min_speakers=1, max_speakers=4)

        # pyannote íƒ€ì„ë¼ì¸ ìˆ˜ì§‘
        pyannote_segments = []
        speaker_durations = {}  # ê° í™”ìì˜ ì´ ë°œí™” ì‹œê°„
        for segment, _, speaker in diarization.itertracks(yield_label=True):
            pyannote_segments.append({
                "start": segment.start,
                "end": segment.end,
                "speaker_id": speaker,
            })
            speaker_durations[speaker] = speaker_durations.get(speaker, 0) + (segment.end - segment.start)

        if not pyannote_segments:
            raise ValueError("pyannote ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

        # êµì‚¬ = ê°€ì¥ ë§ì´ ë°œí™”í•œ í™”ì
        teacher_id = max(speaker_durations, key=speaker_durations.get)

        # Whisper ì„¸ê·¸ë¨¼íŠ¸ â†” pyannote IOU ë§¤ì¹­
        speaker_segs = []
        prev_speaker = None
        interaction_count = 0

        for seg in result.segments:
            seg_start = seg.get("start", 0)
            seg_end = seg.get("end", 0)
            text = seg.get("text", "")

            # IOU ê¸°ë°˜ í™”ì ë§¤ì¹­
            best_speaker_id = teacher_id
            best_overlap = 0

            for ps in pyannote_segments:
                overlap_start = max(seg_start, ps["start"])
                overlap_end = min(seg_end, ps["end"])
                overlap = max(0, overlap_end - overlap_start)

                if overlap > best_overlap:
                    best_overlap = overlap
                    best_speaker_id = ps["speaker_id"]

            speaker = "teacher" if best_speaker_id == teacher_id else "student"
            confidence = min(1.0, best_overlap / max(seg_end - seg_start, 0.01))

            if prev_speaker is not None and speaker != prev_speaker:
                interaction_count += 1
            prev_speaker = speaker

            speaker_segs.append({
                "start": seg_start,
                "end": seg_end,
                "speaker": speaker,
                "speaker_id": best_speaker_id,
                "text": text,
                "confidence": round(confidence, 2),
            })

        result.speaker_segments = speaker_segs
        result.interaction_count = interaction_count
        result.diarization_method = "pyannote"

        # êµì‚¬/í•™ìƒ ë°œí™” ë¹„ìœ¨
        teacher_time = sum(s["end"] - s["start"] for s in speaker_segs if s["speaker"] == "teacher")
        student_time = sum(s["end"] - s["start"] for s in speaker_segs if s["speaker"] == "student")
        total_time = teacher_time + student_time
        result.teacher_ratio = teacher_time / total_time if total_time > 0 else 0.75
        result.student_turns = sum(1 for s in speaker_segs if s["speaker"] == "student")

        print(f"[STT Agent] pyannote í™”ì ë¶„ë¦¬ ì™„ë£Œ: "
              f"{len(speaker_durations)}ëª… ê°ì§€, êµì‚¬={teacher_id}, "
              f"êµì‚¬ë¹„ìœ¨={result.teacher_ratio:.1%}, í•™ìƒë°œí™”={result.student_turns}íšŒ")

    # ================================================================
    # v6.0: íœ´ë¦¬ìŠ¤í‹± í™”ì ë¶„ë¦¬ (í´ë°±)
    # ================================================================

    def _simple_diarization(self, result: STTResult, audio_path: str):
        """
        ì—ë„ˆì§€/ë°œí™” ê¸¸ì´ ê¸°ë°˜ ê²½ëŸ‰ í™”ì ë¶„ë¦¬ (v6.0 ê°œì„ )

        v6.0 ê°œì„ :
        - ì—ë„ˆì§€ ë¹„ìœ¨ ë³€í™” > 1.5ë°° â†’ í™”ì ì „í™˜ í›„ë³´
        - êµì‚¬ ì§§ì€ ì§ˆë¬¸ ì˜¤ë¶„ë¥˜ ë°©ì§€ (ì§ˆë¬¸ íŒ¨í„´ ê°ì§€)
        - í™”ì ë¶„ë¥˜ confidence í•„ë“œ ì¶”ê°€
        """
        try:
            y, sr = librosa.load(audio_path, sr=16000)
        except Exception:
            return

        speaker_segs = []
        prev_speaker = "teacher"
        prev_energy = 0.0
        interaction_count = 0

        # êµì‚¬ ì§ˆë¬¸ íŒ¨í„´ (ì§§ì§€ë§Œ í•™ìƒì´ ì•„ë‹Œ ê²½ìš°)
        teacher_question_patterns = [
            "ë§ì•„", "ë§ì£ ", "ê·¸ë ‡ì£ ", "ì•„ë‹ˆì•¼", "ì´í•´í–ˆì£ ",
            "í•´ë³¼ê¹Œ", "í•´ë³´ì„¸ìš”", "ì½ì–´ë´", "ë°œí‘œí•´",
            "ë­ì˜ˆìš”", "ëˆ„ê°€", "ì–´ë–¤", "ì´ê±´ ë­", "ì•Œê²€",
            "ë‹¤ê°™ì´", "í•¨ê»˜", "ì—¬ëŸ¬ë¶„", "ë°”ë¡œ",
        ]

        for seg in result.segments:
            start = seg.get("start", 0)
            end = seg.get("end", 0)
            text = seg.get("text", "")
            seg_duration = end - start

            # ì—ë„ˆì§€ ê³„ì‚°
            start_sample = int(start * sr)
            end_sample = min(int(end * sr), len(y))
            if end_sample <= start_sample:
                continue
            segment_audio = y[start_sample:end_sample]
            energy = float(np.sqrt(np.mean(segment_audio ** 2)))

            # v6.0: ì—ë„ˆì§€ ë¹„ìœ¨ ë³€í™” ê°ì§€
            energy_ratio = energy / prev_energy if prev_energy > 0.001 else 1.0

            # v6.0: êµì‚¬ ì§ˆë¬¸ íŒ¨í„´ ê²€ì‚¬ (ì§§ì§€ë§Œ êµì‚¬ ë°œí™”ì¸ ê²½ìš°)
            is_teacher_question = any(p in text for p in teacher_question_patterns)

            # í™”ì íŒë³„ ê·œì¹™ (v6.0 ê°œì„ )
            confidence = 0.5  # ê¸°ë³¸ í™•ì‹ ë„

            if seg_duration < 1.5 and len(text.split()) < 5 and not is_teacher_question:
                # ë§¤ìš° ì§§ì€ ì‘ë‹µ + êµì‚¬ ì§ˆë¬¸ ì•„ë‹˜ â†’ í•™ìƒ
                speaker = "student"
                confidence = 0.7
            elif seg_duration < 2.5 and self._is_response_pattern(text) and not is_teacher_question:
                # ì§§ì€ ì‘ë‹µ íŒ¨í„´ â†’ í•™ìƒ
                speaker = "student"
                confidence = 0.65
            elif energy_ratio > 1.8 and seg_duration < 3.0 and not is_teacher_question:
                # v6.0: ì—ë„ˆì§€ ê¸‰ë³€ + ì§§ì€ ë°œí™” â†’ í™”ì ì „í™˜ (í•™ìƒ)
                speaker = "student"
                confidence = 0.55
            elif is_teacher_question:
                # êµì‚¬ ì§ˆë¬¸ íŒ¨í„´ â†’ êµì‚¬ (ì§§ì•„ë„)
                speaker = "teacher"
                confidence = 0.8
            else:
                speaker = "teacher"
                confidence = 0.75

            # êµëŒ€ íšŸìˆ˜ ì¹´ìš´íŠ¸
            if speaker != prev_speaker:
                interaction_count += 1
            prev_speaker = speaker
            prev_energy = energy

            speaker_segs.append({
                "start": start,
                "end": end,
                "speaker": speaker,
                "text": text,
                "energy": round(energy, 4),
                "confidence": round(confidence, 2),  # v6.0 ì¶”ê°€
            })

        result.speaker_segments = speaker_segs
        result.interaction_count = interaction_count

        # êµì‚¬/í•™ìƒ ë°œí™” ë¹„ìœ¨
        teacher_time = sum(
            s["end"] - s["start"] for s in speaker_segs if s["speaker"] == "teacher"
        )
        student_time = sum(
            s["end"] - s["start"] for s in speaker_segs if s["speaker"] == "student"
        )
        total_time = teacher_time + student_time
        result.teacher_ratio = teacher_time / total_time if total_time > 0 else 0.75
        result.student_turns = sum(1 for s in speaker_segs if s["speaker"] == "student")
        result.diarization_method = "heuristic"

    def _is_response_pattern(self, text: str) -> bool:
        """í•™ìƒ ì‘ë‹µ íŒ¨í„´ ê°ì§€"""
        response_patterns = [
            "ë„¤", "ì˜ˆ", "ì•„", "ë§ì•„ìš”", "ì•Œê² ìŠµë‹ˆë‹¤", "ê°ì‚¬í•©ë‹ˆë‹¤",
            "ì„ ìƒë‹˜", "ì €ìš”", "ì—¬ê¸°ìš”", "ë‹¤ì„¯", "í•˜ë‚˜", "ë‘˜", "ì…‹", "ë„·",
        ]
        text_stripped = text.strip()
        for pat in response_patterns:
            if text_stripped == pat or text_stripped.startswith(pat + " "):
                return True
        return len(text_stripped) < 10

    # ================================================================
    # v5.0: ì§ˆë¬¸ ê°ì§€
    # ================================================================

    def _detect_questions(self, result: STTResult):
        """ë°œí™”ì—ì„œ ì§ˆë¬¸ íšŸìˆ˜ ê°ì§€"""
        if not result.transcript:
            return

        # ë¬¼ìŒí‘œ ê¸°ë°˜
        q_mark_count = result.transcript.count("?")

        # í•œêµ­ì–´ ì§ˆë¬¸ íŒ¨í„´ (? ì—†ëŠ” ê²½ìš°ë„ ê°ì§€)
        question_patterns = [
            r'[ê°€-í£]+\s*(?:í• ê¹Œìš”|í• ë˜ìš”|í• ê²Œìš”|í•´ë³¼ê¹Œ)',
            r'[ê°€-í£]+\s*(?:ì¼ê¹Œìš”|ì¸ê°€ìš”|ë‚˜ìš”|ê¹Œìš”|ã„¹ê¹Œ)',
            r'ë­ê°€|ì–´ë–¤|ì™œ|ì–´ë–»ê²Œ|ëª‡\s*(?:ê°œ|ëª…|ë²ˆ)',
            r'ì•Œê² [ì§€ì£ ]|ì´í•´[í–ˆí•˜]|ë§[ì§€ì£ ë‚˜]',
        ]

        pattern_count = 0
        for pat in question_patterns:
            pattern_count += len(re.findall(pat, result.transcript))

        result.question_count = max(q_mark_count, pattern_count)

    # ================================================================
    # ê¸°ì¡´ ìœ í‹¸ë¦¬í‹°
    # ================================================================

    def _detect_fillers(self, result: STTResult):
        """í•œêµ­ì–´ í•„ëŸ¬ ë‹¨ì–´ ê°ì§€"""
        if not result.transcript:
            return

        text = result.transcript
        filler_counts = {}
        total_fillers = 0

        for filler in KOREAN_FILLER_WORDS:
            count = len(re.findall(rf'\b{re.escape(filler)}\b', text))
            if count > 0:
                filler_counts[filler] = count
                total_fillers += count

        result.filler_words = filler_counts
        if result.word_count > 0:
            result.filler_ratio = total_fillers / result.word_count

    def _get_audio_duration(self, path: str) -> float:
        """FFprobeë¡œ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¡°íšŒ"""
        try:
            cmd = [
                "ffprobe", "-v", "quiet", "-show_entries",
                "format=duration", "-of", "csv=p=0", path
            ]
            out = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
            return float(out.stdout.strip())
        except Exception:
            return 0.0

    def _get_video_duration(self, path: str) -> float:
        """FFprobeë¡œ ë¹„ë””ì˜¤ ê¸¸ì´ ì¡°íšŒ"""
        return self._get_audio_duration(path)
