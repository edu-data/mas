"""
ğŸ—£ï¸ STT Agent - ìŒì„±â†’í…ìŠ¤íŠ¸ ë³€í™˜ ì „ë¬¸ ì—ì´ì „íŠ¸
Whisper/Faster-Whisper ê¸°ë°˜ STT ë° í´ë°± ë¶„ì„ ì§€ì›
"""

import re
import subprocess
from pathlib import Path
from typing import Dict, Optional, List
from dataclasses import dataclass, field


# ì„ íƒì  ì˜ì¡´ì„±
try:
    import whisper
    HAS_WHISPER = True
except ImportError:
    HAS_WHISPER = False

try:
    from faster_whisper import WhisperModel
    HAS_FASTER_WHISPER = True
except ImportError:
    HAS_FASTER_WHISPER = False


# í•œêµ­ì–´ êµìœ¡í•™ í•„ëŸ¬ ë‹¨ì–´
KOREAN_FILLER_WORDS = [
    "ìŒ", "ì–´", "ì´ì œ", "ê·¸ë˜ì„œ", "ì", "ë„¤", "ì˜ˆ",
    "ê·¸ëŸ¬ë‹ˆê¹Œ", "ë­", "ì•½ê°„", "ì¢€", "í•œë²ˆ", "ê·¸ëƒ¥"
]


@dataclass
class STTResult:
    """STT ë¶„ì„ ê²°ê³¼"""
    transcript: str = ""
    word_count: int = 0
    speaking_rate: float = 0.0        # WPM (Words Per Minute)
    duration_seconds: float = 0.0
    filler_words: Dict[str, int] = field(default_factory=dict)
    filler_ratio: float = 0.0
    language: str = "ko"
    segments: List[Dict] = field(default_factory=list)
    confidence: float = 0.0
    method: str = "fallback"           # whisper / faster_whisper / fallback

    def to_dict(self) -> Dict:
        return {
            "transcript": self.transcript[:500] if self.transcript else "",
            "transcript_length": len(self.transcript),
            "word_count": self.word_count,
            "speaking_rate": round(self.speaking_rate, 1),
            "speaking_pattern": self._classify_speaking_pattern(),
            "duration_seconds": round(self.duration_seconds, 1),
            "filler_words": self.filler_words,
            "filler_ratio": round(self.filler_ratio, 3),
            "filler_count": sum(self.filler_words.values()),
            "language": self.language,
            "segment_count": len(self.segments),
            "confidence": round(self.confidence, 2),
            "method": self.method,
        }

    def _classify_speaking_pattern(self) -> str:
        """ë§í•˜ê¸° íŒ¨í„´ ë¶„ë¥˜"""
        if self.speaking_rate < 80:
            return "ëŠë¦¼ (Slow)"
        elif self.speaking_rate < 120:
            return "ëŒ€í™”í˜• (Conversational)"
        elif self.speaking_rate < 160:
            return "ê°•ì˜í˜• (Lecture)"
        else:
            return "ë¹ ë¦„ (Fast)"


class STTAgent:
    """
    ğŸ—£ï¸ STT Agent
    ìŒì„± ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ì–¸ì–´ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.

    ì§€ì› ì—”ì§„:
    1. Faster-Whisper (ìµœìš°ì„  - 3x ì†ë„)
    2. OpenAI Whisper (í´ë°±)
    3. FFprobe ê¸°ë°˜ ë©”íƒ€ë°ì´í„° ë¶„ì„ (ML ë¯¸ì„¤ì¹˜ í™˜ê²½)
    """

    def __init__(self, model_size: str = "base", language: str = "ko"):
        self.model_size = model_size
        self.language = language
        self._whisper_model = None
        self._faster_model = None

    def analyze(self, audio_path: str) -> Dict:
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ ë¶„ì„

        Args:
            audio_path: WAV/MP3 ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ

        Returns:
            STT ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        path = Path(audio_path)
        if not path.exists():
            return STTResult(method="error").to_dict()

        duration = self._get_audio_duration(str(path))

        # ì—”ì§„ ìš°ì„ ìˆœìœ„: Faster-Whisper > Whisper > Fallback
        if HAS_FASTER_WHISPER:
            result = self._analyze_faster_whisper(str(path), duration)
        elif HAS_WHISPER:
            result = self._analyze_whisper(str(path), duration)
        else:
            result = self._analyze_fallback(str(path), duration)

        return result.to_dict()

    def analyze_from_video(self, video_path: str) -> Dict:
        """
        ë¹„ë””ì˜¤ì—ì„œ ì§ì ‘ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¶„ì„

        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ

        Returns:
            STT ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        import tempfile
        import os

        temp_audio = os.path.join(tempfile.gettempdir(), "gaim_stt_temp.wav")

        try:
            subprocess.run([
                "ffmpeg", "-y", "-i", video_path,
                "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1",
                temp_audio
            ], capture_output=True, timeout=120)

            if Path(temp_audio).exists():
                return self.analyze(temp_audio)
        except Exception:
            pass

        # FFprobe í´ë°±
        duration = self._get_video_duration(video_path)
        return self._analyze_fallback(video_path, duration).to_dict()

    def _analyze_faster_whisper(self, audio_path: str, duration: float) -> STTResult:
        """Faster-Whisper ì—”ì§„ìœ¼ë¡œ STT ìˆ˜í–‰"""
        try:
            if self._faster_model is None:
                self._faster_model = WhisperModel(
                    self.model_size, device="cpu", compute_type="int8"
                )

            segments, info = self._faster_model.transcribe(
                audio_path, language=self.language, beam_size=5
            )

            all_text = []
            seg_list = []
            for seg in segments:
                all_text.append(seg.text)
                seg_list.append({
                    "start": round(seg.start, 2),
                    "end": round(seg.end, 2),
                    "text": seg.text.strip(),
                })

            transcript = " ".join(all_text)
            word_count = len(transcript.split())

            result = STTResult(
                transcript=transcript,
                word_count=word_count,
                speaking_rate=round(word_count / (duration / 60), 1) if duration > 0 else 0,
                duration_seconds=duration,
                language=self.language,
                segments=seg_list,
                confidence=0.85,
                method="faster_whisper",
            )
            self._detect_fillers(result)
            return result

        except Exception as e:
            print(f"[STT Agent] Faster-Whisper ì˜¤ë¥˜: {e}")
            return self._analyze_fallback(audio_path, duration)

    def _analyze_whisper(self, audio_path: str, duration: float) -> STTResult:
        """OpenAI Whisper ì—”ì§„ìœ¼ë¡œ STT ìˆ˜í–‰"""
        try:
            if self._whisper_model is None:
                self._whisper_model = whisper.load_model(self.model_size)

            result = self._whisper_model.transcribe(
                audio_path, language=self.language
            )

            transcript = result.get("text", "")
            word_count = len(transcript.split())
            segments = [
                {
                    "start": round(s["start"], 2),
                    "end": round(s["end"], 2),
                    "text": s["text"].strip(),
                }
                for s in result.get("segments", [])
            ]

            stt_result = STTResult(
                transcript=transcript,
                word_count=word_count,
                speaking_rate=round(word_count / (duration / 60), 1) if duration > 0 else 0,
                duration_seconds=duration,
                language=self.language,
                segments=segments,
                confidence=0.80,
                method="whisper",
            )
            self._detect_fillers(stt_result)
            return stt_result

        except Exception as e:
            print(f"[STT Agent] Whisper ì˜¤ë¥˜: {e}")
            return self._analyze_fallback(audio_path, duration)

    def _analyze_fallback(self, file_path: str, duration: float) -> STTResult:
        """ML ì—†ì´ FFprobe ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¶”ì • ë¶„ì„"""
        if duration <= 0:
            duration = self._get_audio_duration(file_path) or 600.0

        # í•œêµ­ì–´ í‰ê·  ë°œí™” ì†ë„: ~120 WPM (ì¶”ì •)
        estimated_wpm = 125.0
        speaking_ratio = 0.75  # ì¼ë°˜ì ìœ¼ë¡œ ìˆ˜ì—… ì‹œê°„ì˜ 75%ê°€ ë°œí™”

        estimated_word_count = int(estimated_wpm * (duration / 60) * speaking_ratio)

        return STTResult(
            transcript="[í´ë°± ëª¨ë“œ: ì‹¤ì œ í…ìŠ¤íŠ¸ ë³€í™˜ ì—†ìŒ. Whisper ì„¤ì¹˜ í•„ìš”]",
            word_count=estimated_word_count,
            speaking_rate=estimated_wpm,
            duration_seconds=duration,
            filler_words={},
            filler_ratio=0.03,  # í‰ê·  ì¶”ì •
            language=self.language,
            confidence=0.40,
            method="fallback",
        )

    def _detect_fillers(self, result: STTResult):
        """í•œêµ­ì–´ í•„ëŸ¬ ë‹¨ì–´ ê°ì§€"""
        if not result.transcript:
            return

        text = result.transcript
        filler_counts = {}
        total_fillers = 0

        for filler in KOREAN_FILLER_WORDS:
            count = len(re.findall(rf'\b{re.escape(filler)}\b', text))
            if count > 0:
                filler_counts[filler] = count
                total_fillers += count

        result.filler_words = filler_counts
        if result.word_count > 0:
            result.filler_ratio = total_fillers / result.word_count

    def _get_audio_duration(self, path: str) -> float:
        """FFprobeë¡œ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¡°íšŒ"""
        try:
            cmd = [
                "ffprobe", "-v", "quiet", "-show_entries",
                "format=duration", "-of", "csv=p=0", path
            ]
            out = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
            return float(out.stdout.strip())
        except Exception:
            return 0.0

    def _get_video_duration(self, path: str) -> float:
        """FFprobeë¡œ ë¹„ë””ì˜¤ ê¸¸ì´ ì¡°íšŒ"""
        return self._get_audio_duration(path)
