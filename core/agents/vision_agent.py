"""
ğŸ‘ï¸ Vision Agent - ê°•ì‚¬ ë¹„ì–¸ì–´ í–‰ë™ ë¶„ì„
v5.0: MediaPipe Face Mesh + Pose Estimation ê¸°ë°˜ (OpenCV í´ë°± ìœ ì§€)
"""

import cv2
import numpy as np
from dataclasses import dataclass
from typing import List, Dict, Optional
from pathlib import Path

# MediaPipe graceful import
try:
    import mediapipe as mp
    HAS_MEDIAPIPE = True
except ImportError:
    HAS_MEDIAPIPE = False


@dataclass
class VisionMetrics:
    """ë¹„ì „ ë¶„ì„ ê²°ê³¼ ë©”íŠ¸ë¦­"""
    timestamp: float
    gesture_active: bool = False           # ì œìŠ¤ì²˜ í™œì„±í™” ì—¬ë¶€
    gesture_score: float = 0.0             # ì œìŠ¤ì²˜ ì—­ë™ì„± ì ìˆ˜ (0-100)
    hand_position: str = "unknown"         # low, mid, high
    eye_contact: bool = False              # ì •ë©´ ì‘ì‹œ ì—¬ë¶€
    face_detected: bool = False            # ì–¼êµ´ ê°ì§€ ì—¬ë¶€
    expression_score: float = 50.0         # í‘œì • í™œë ¥ ì ìˆ˜ (0-100)
    body_openness: float = 0.5             # ìì„¸ ê°œë°©ì„± (0-1)
    motion_score: float = 0.0              # ì›€ì§ì„ ì ìˆ˜
    # v5.0 ì¶”ê°€ í•„ë“œ
    gaze_direction: str = "center"         # left, center, right
    shoulder_angle: float = 0.0            # ì–´ê¹¨ ê°ë„ (ìì„¸)
    arm_gesture_type: str = "none"         # none, pointing, open, closed
    head_tilt: float = 0.0                 # ë¨¸ë¦¬ ê¸°ìš¸ê¸° (ë„)


class VisionAgent:
    """
    ğŸ‘ï¸ Vision Agent v5.0
    ê°•ì‚¬ì˜ ë¹„ì–¸ì–´ì  ìš”ì†Œ(ì œìŠ¤ì²˜, ì‹œì„ , í‘œì •)ë¥¼ ë¶„ì„

    v5.0: MediaPipe Face Mesh(468 landmark) + Pose(33 joints)
    í´ë°±: OpenCV Haar Cascade (MediaPipe ë¯¸ì„¤ì¹˜ í™˜ê²½)
    """

    def __init__(self, config: Optional[dict] = None):
        self.config = config or {
            "gesture_threshold": 0.6,
            "face_confidence": 0.5
        }

        self.use_mediapipe = HAS_MEDIAPIPE

        if self.use_mediapipe:
            self._init_mediapipe()
        else:
            self._init_opencv_fallback()

        # ì›€ì§ì„ ê°ì§€ë¥¼ ìœ„í•œ ì´ì „ í”„ë ˆì„
        self.prev_frame = None
        self.prev_gray = None
        self.prev_landmarks = None

        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.results: List[VisionMetrics] = []

    def _init_mediapipe(self):
        """MediaPipe ì´ˆê¸°í™”"""
        self.mp_face_mesh = mp.solutions.face_mesh
        self.mp_pose = mp.solutions.pose

        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=True,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5
        )
        self.pose = self.mp_pose.Pose(
            static_image_mode=True,
            model_complexity=1,
            min_detection_confidence=0.5
        )

    def _init_opencv_fallback(self):
        """OpenCV í´ë°± ì´ˆê¸°í™”"""
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )

    def analyze_frame(self, frame: np.ndarray, timestamp: float) -> VisionMetrics:
        """
        ë‹¨ì¼ í”„ë ˆì„ì˜ ë¹„ì–¸ì–´ì  ìš”ì†Œ ë¶„ì„

        Args:
            frame: BGR ì´ë¯¸ì§€ (OpenCV í˜•ì‹)
            timestamp: í”„ë ˆì„ íƒ€ì„ìŠ¤íƒ¬í”„ (ì´ˆ)

        Returns:
            VisionMetrics ê°ì²´
        """
        metrics = VisionMetrics(timestamp=timestamp)

        if self.use_mediapipe:
            self._analyze_mediapipe(frame, metrics)
        else:
            self._analyze_opencv(frame, metrics)

        # ê³µí†µ: ì›€ì§ì„ ë¶„ì„
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        self._analyze_motion(gray, metrics)

        self.prev_frame = frame.copy()
        self.prev_gray = gray.copy()

        self.results.append(metrics)
        return metrics

    # ================================================================
    # MediaPipe ë¶„ì„ (v5.0)
    # ================================================================

    def _analyze_mediapipe(self, frame: np.ndarray, metrics: VisionMetrics):
        """MediaPipe ê¸°ë°˜ ì¢…í•© ë¶„ì„"""
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        h, w = frame.shape[:2]

        # 1. Face Mesh ë¶„ì„
        face_result = self.face_mesh.process(rgb)
        if face_result.multi_face_landmarks:
            landmarks = face_result.multi_face_landmarks[0]
            metrics.face_detected = True

            # ì‹œì„  ë°©í–¥ (í™ì±„ ëœë“œë§ˆí¬ ê¸°ë°˜)
            self._analyze_gaze_mediapipe(landmarks, w, h, metrics)

            # í‘œì • í™œë ¥ (ì–¼êµ´ ëœë“œë§ˆí¬ ë³€í™”ëŸ‰)
            self._analyze_expression_mediapipe(landmarks, metrics)

            # ë¨¸ë¦¬ ê¸°ìš¸ê¸°
            self._analyze_head_tilt(landmarks, h, metrics)

            self.prev_landmarks = landmarks

        # 2. Pose ë¶„ì„
        pose_result = self.pose.process(rgb)
        if pose_result.pose_landmarks:
            pose = pose_result.pose_landmarks
            self._analyze_pose_mediapipe(pose, w, h, metrics)

    def _analyze_gaze_mediapipe(self, landmarks, w, h, metrics: VisionMetrics):
        """í™ì±„ ëœë“œë§ˆí¬ ê¸°ë°˜ ì‹œì„  ë°©í–¥ ë¶„ì„"""
        # MediaPipe Face Mesh í™ì±„ ì¸ë±ìŠ¤
        # ì™¼ëˆˆ í™ì±„: 468-472, ì˜¤ë¥¸ëˆˆ í™ì±„: 473-477
        try:
            left_iris = landmarks.landmark[468]   # ì™¼ëˆˆ í™ì±„ ì¤‘ì‹¬
            right_iris = landmarks.landmark[473]  # ì˜¤ë¥¸ëˆˆ í™ì±„ ì¤‘ì‹¬
            nose = landmarks.landmark[1]          # ì½” ë

            # í™ì±„ ì¤‘ì‹¬ ëŒ€ë¹„ ì½” ìœ„ì¹˜ë¡œ ì‹œì„  ë°©í–¥ íŒë‹¨
            iris_center_x = (left_iris.x + right_iris.x) / 2
            nose_x = nose.x

            gaze_offset = iris_center_x - nose_x

            if abs(gaze_offset) < 0.015:
                metrics.gaze_direction = "center"
                metrics.eye_contact = True
            elif gaze_offset < -0.015:
                metrics.gaze_direction = "left"
                metrics.eye_contact = False
            else:
                metrics.gaze_direction = "right"
                metrics.eye_contact = False

            # ì½”ê°€ í™”ë©´ ì¤‘ì•™ì— ê°€ê¹Œìš°ë©´ ì •ë©´ ì‘ì‹œ
            if 0.3 < nose_x < 0.7 and 0.3 < nose.y < 0.7:
                metrics.eye_contact = True

        except (IndexError, AttributeError):
            # ëœë“œë§ˆí¬ ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ìœ ì§€
            pass

    def _analyze_expression_mediapipe(self, landmarks, metrics: VisionMetrics):
        """ì–¼êµ´ ëœë“œë§ˆí¬ ë³€í™”ëŸ‰ìœ¼ë¡œ í‘œì • í™œë ¥ ì¸¡ì •"""
        if self.prev_landmarks is None:
            metrics.expression_score = 50.0
            return

        try:
            # ì£¼ìš” ì–¼êµ´ í¬ì¸íŠ¸ ë³€í™”ëŸ‰ ì¸¡ì • (ì…, ëˆˆì¹, ë³¼)
            key_indices = [
                61, 291,    # ì… ì¢Œìš°
                0, 17,      # ì… ìƒí•˜
                70, 300,    # ëˆˆì¹ ì¢Œìš°
                152, 10,    # í„±, ì´ë§ˆ
            ]

            total_movement = 0.0
            for idx in key_indices:
                curr = landmarks.landmark[idx]
                prev = self.prev_landmarks.landmark[idx]
                dx = curr.x - prev.x
                dy = curr.y - prev.y
                total_movement += (dx * dx + dy * dy) ** 0.5

            # ë³€í™”ëŸ‰ì„ 0~100 ì ìˆ˜ë¡œ ë³€í™˜
            metrics.expression_score = min(100, total_movement * 5000)
        except (IndexError, AttributeError):
            metrics.expression_score = 50.0

    def _analyze_head_tilt(self, landmarks, h, metrics: VisionMetrics):
        """ë¨¸ë¦¬ ê¸°ìš¸ê¸° ë¶„ì„"""
        try:
            left_ear = landmarks.landmark[234]
            right_ear = landmarks.landmark[454]

            dy = right_ear.y - left_ear.y
            dx = right_ear.x - left_ear.x

            angle = np.degrees(np.arctan2(dy, dx))
            metrics.head_tilt = round(angle, 1)
        except (IndexError, AttributeError):
            pass

    def _analyze_pose_mediapipe(self, pose, w, h, metrics: VisionMetrics):
        """MediaPipe Pose ê¸°ë°˜ ì œìŠ¤ì²˜/ìì„¸ ë¶„ì„"""
        lm = pose.landmark

        # ì–´ê¹¨ ê°ë„ (ìì„¸ ê°œë°©ì„±)
        try:
            l_shoulder = lm[11]
            r_shoulder = lm[12]
            shoulder_width = abs(l_shoulder.x - r_shoulder.x)
            metrics.shoulder_angle = shoulder_width
            metrics.body_openness = min(1.0, shoulder_width * 3.0)
        except IndexError:
            pass

        # ì† ìœ„ì¹˜
        try:
            l_wrist = lm[15]
            r_wrist = lm[16]
            l_shoulder = lm[11]
            r_shoulder = lm[12]

            # ì†ëª©-ì–´ê¹¨ Yì¢Œí‘œ ë¹„êµ
            avg_wrist_y = (l_wrist.y + r_wrist.y) / 2
            avg_shoulder_y = (l_shoulder.y + r_shoulder.y) / 2

            if avg_wrist_y < avg_shoulder_y - 0.1:
                metrics.hand_position = "high"
            elif avg_wrist_y > avg_shoulder_y + 0.2:
                metrics.hand_position = "low"
            else:
                metrics.hand_position = "mid"
        except IndexError:
            pass

        # íŒ” ì œìŠ¤ì²˜ ìœ í˜• ë¶„ë¥˜
        try:
            l_elbow = lm[13]
            r_elbow = lm[14]
            l_wrist = lm[15]
            r_wrist = lm[16]

            # íŒ” ë²Œë¦¼ ì •ë„
            arm_spread = abs(l_wrist.x - r_wrist.x)
            arm_height = avg_shoulder_y - avg_wrist_y

            if arm_spread > 0.5:
                metrics.arm_gesture_type = "open"
                metrics.gesture_active = True
                metrics.gesture_score = min(100, arm_spread * 150)
            elif arm_height > 0.15:
                metrics.arm_gesture_type = "pointing"
                metrics.gesture_active = True
                metrics.gesture_score = min(100, arm_height * 300)
            elif arm_spread < 0.15:
                metrics.arm_gesture_type = "closed"
                metrics.gesture_active = False
                metrics.gesture_score = 10.0
            else:
                metrics.arm_gesture_type = "none"
                metrics.gesture_active = arm_spread > 0.25
                metrics.gesture_score = min(100, arm_spread * 100)
        except (IndexError, UnboundLocalError):
            pass

    # ================================================================
    # OpenCV í´ë°± (v4.x í˜¸í™˜)
    # ================================================================

    def _analyze_opencv(self, frame: np.ndarray, metrics: VisionMetrics):
        """OpenCV ê¸°ë°˜ ê°„ì†Œí™” ë¶„ì„ (MediaPipe ë¯¸ì„¤ì¹˜ í™˜ê²½)"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        height, width = frame.shape[:2]

        # ì–¼êµ´ ê°ì§€
        faces = self.face_cascade.detectMultiScale(
            gray, scaleFactor=1.1, minNeighbors=4, minSize=(30, 30)
        )

        if len(faces) > 0:
            metrics.face_detected = True
            largest_face = max(faces, key=lambda f: f[2] * f[3])
            x, y, w, h = largest_face

            face_center_x = (x + w // 2) / width
            is_centered = 0.3 < face_center_x < 0.7
            metrics.eye_contact = is_centered
            metrics.gaze_direction = "center" if is_centered else ("left" if face_center_x < 0.3 else "right")

            face_size_ratio = (w * h) / (width * height)
            metrics.expression_score = min(100, face_size_ratio * 1000)

        # ì˜ì—­ë³„ ë¶„ì„
        self._analyze_regions_opencv(frame, metrics)

    def _analyze_regions_opencv(self, frame: np.ndarray, metrics: VisionMetrics):
        """í”„ë ˆì„ ì˜ì—­ë³„ ë¶„ì„ (í´ë°±)"""
        height, width = frame.shape[:2]

        top_half = frame[:height//2, :]
        bottom_half = frame[height//2:, :]

        top_edges = cv2.Canny(cv2.cvtColor(top_half, cv2.COLOR_BGR2GRAY), 50, 150)
        bottom_edges = cv2.Canny(cv2.cvtColor(bottom_half, cv2.COLOR_BGR2GRAY), 50, 150)

        top_activity = np.count_nonzero(top_edges) / top_edges.size
        bottom_activity = np.count_nonzero(bottom_edges) / bottom_edges.size

        if top_activity > bottom_activity * 1.5:
            metrics.hand_position = "high"
        elif bottom_activity > top_activity * 1.5:
            metrics.hand_position = "low"
        else:
            metrics.hand_position = "mid"

        metrics.body_openness = min(1.0, (top_activity + bottom_activity) * 10)

    # ================================================================
    # ê³µí†µ ë¶„ì„
    # ================================================================

    def _analyze_motion(self, gray: np.ndarray, metrics: VisionMetrics):
        """ì›€ì§ì„ ë¶„ì„"""
        if self.prev_gray is None:
            return

        diff = cv2.absdiff(gray, self.prev_gray)
        _, thresh = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)

        motion_pixels = np.count_nonzero(thresh)
        total_pixels = thresh.size
        motion_ratio = motion_pixels / total_pixels

        metrics.motion_score = min(100, motion_ratio * 500)

        # MediaPipeê°€ ì œìŠ¤ì²˜ë¥¼ ë¶„ì„í•˜ì§€ ëª»í•œ ê²½ìš° ëª¨ì…˜ ê¸°ë°˜ í´ë°±
        if not metrics.gesture_active:
            metrics.gesture_active = motion_ratio > 0.02
            if metrics.gesture_score == 0:
                metrics.gesture_score = metrics.motion_score

    def get_summary(self) -> Dict:
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½"""
        if not self.results:
            return {"error": "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}

        total = len(self.results)

        summary = {
            "total_frames_analyzed": total,
            "analysis_engine": "mediapipe" if self.use_mediapipe else "opencv",
            "gesture_active_ratio": sum(1 for r in self.results if r.gesture_active) / total,
            "avg_gesture_score": float(np.mean([r.gesture_score for r in self.results])),
            "eye_contact_ratio": sum(1 for r in self.results if r.eye_contact) / total,
            "face_detection_ratio": sum(1 for r in self.results if r.face_detected) / total,
            "avg_expression_score": float(np.mean([r.expression_score for r in self.results])),
            "avg_body_openness": float(np.mean([r.body_openness for r in self.results])),
            "avg_motion_score": float(np.mean([r.motion_score for r in self.results])),
            "hand_position_distribution": self._get_hand_distribution(),
        }

        # v5.0 ì¶”ê°€ ìš”ì•½
        if self.use_mediapipe:
            summary["gaze_distribution"] = self._get_gaze_distribution()
            summary["gesture_type_distribution"] = self._get_gesture_type_distribution()
            summary["avg_head_tilt"] = float(np.mean([abs(r.head_tilt) for r in self.results]))

        return summary

    def _get_hand_distribution(self) -> Dict[str, float]:
        """ì† ìœ„ì¹˜ ë¶„í¬"""
        if not self.results:
            return {}
        positions = [r.hand_position for r in self.results]
        total = len(positions)
        return {
            "high": positions.count("high") / total,
            "mid": positions.count("mid") / total,
            "low": positions.count("low") / total
        }

    def _get_gaze_distribution(self) -> Dict[str, float]:
        """ì‹œì„  ë°©í–¥ ë¶„í¬ (v5.0)"""
        if not self.results:
            return {}
        gazes = [r.gaze_direction for r in self.results]
        total = len(gazes)
        return {
            "left": gazes.count("left") / total,
            "center": gazes.count("center") / total,
            "right": gazes.count("right") / total,
        }

    def _get_gesture_type_distribution(self) -> Dict[str, float]:
        """ì œìŠ¤ì²˜ ìœ í˜• ë¶„í¬ (v5.0)"""
        if not self.results:
            return {}
        types = [r.arm_gesture_type for r in self.results]
        total = len(types)
        return {
            "open": types.count("open") / total,
            "pointing": types.count("pointing") / total,
            "closed": types.count("closed") / total,
            "none": types.count("none") / total,
        }

    def get_timeline(self) -> List[Dict]:
        """ì‹œê°„ë³„ ë¶„ì„ ê²°ê³¼"""
        return [
            {
                "timestamp": r.timestamp,
                "gesture_score": r.gesture_score,
                "eye_contact": r.eye_contact,
                "expression_score": r.expression_score,
                "motion_score": r.motion_score,
                "gaze_direction": r.gaze_direction,
                "arm_gesture_type": r.arm_gesture_type,
            }
            for r in self.results
        ]

    def reset(self):
        """ë¶„ì„ ê²°ê³¼ ì´ˆê¸°í™”"""
        self.results = []
        self.prev_frame = None
        self.prev_gray = None
        self.prev_landmarks = None
