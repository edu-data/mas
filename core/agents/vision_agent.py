"""
ğŸ‘ï¸ Vision Agent - ê°•ì‚¬ ë¹„ì–¸ì–´ í–‰ë™ ë¶„ì„
v6.0: MediaPipe Tasks API (FaceLandmarker + PoseLandmarker) ë§ˆì´ê·¸ë ˆì´ì…˜
     mp.solutions ì œê±° â†’ mp.tasks.vision ì‚¬ìš©
"""

import cv2
import numpy as np
from dataclasses import dataclass
from typing import List, Dict, Optional
from pathlib import Path

# MediaPipe Tasks API (v0.10+)
try:
    import mediapipe as mp
    _MP_VIS = mp.tasks.vision
    _BASE_OPTS = mp.tasks.BaseOptions
    _MODEL_DIR = Path(mp.__file__).parent / "models"
    HAS_MEDIAPIPE = (_MODEL_DIR / "face_landmarker.task").exists()
except (ImportError, AttributeError):
    HAS_MEDIAPIPE = False


@dataclass
class VisionMetrics:
    """ë¹„ì „ ë¶„ì„ ê²°ê³¼ ë©”íŠ¸ë¦­"""
    timestamp: float
    gesture_active: bool = False
    gesture_score: float = 0.0
    hand_position: str = "unknown"
    eye_contact: bool = False
    face_detected: bool = False
    expression_score: float = 50.0
    body_openness: float = 0.5
    motion_score: float = 0.0
    gaze_direction: str = "center"
    shoulder_angle: float = 0.0
    arm_gesture_type: str = "none"
    head_tilt: float = 0.0


class VisionAgent:
    """
    ğŸ‘ï¸ Vision Agent v6.0
    ê°•ì‚¬ì˜ ë¹„ì–¸ì–´ì  ìš”ì†Œ(ì œìŠ¤ì²˜, ì‹œì„ , í‘œì •)ë¥¼ ë¶„ì„

    v6.0: MediaPipe Tasks API (FaceLandmarker + PoseLandmarker)
    í´ë°±: OpenCV Haar Cascade (MediaPipe ë¯¸ì„¤ì¹˜ í™˜ê²½)
    """

    def __init__(self, config: Optional[dict] = None):
        self.config = config or {
            "gesture_threshold": 0.6,
            "face_confidence": 0.5
        }

        self.use_mediapipe = HAS_MEDIAPIPE

        if self.use_mediapipe:
            self._init_mediapipe()
        else:
            self._init_opencv_fallback()

        # ì›€ì§ì„ ê°ì§€ë¥¼ ìœ„í•œ ì´ì „ í”„ë ˆì„
        self.prev_frame = None
        self.prev_gray = None
        self.prev_landmarks = None

        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.results: List[VisionMetrics] = []

    def _init_mediapipe(self):
        """MediaPipe Tasks API ì´ˆê¸°í™”"""
        face_model = str(_MODEL_DIR / "face_landmarker.task")
        pose_model = str(_MODEL_DIR / "pose_landmarker_lite.task")

        # FaceLandmarker ìƒì„±
        face_opts = _MP_VIS.FaceLandmarkerOptions(
            base_options=_BASE_OPTS(model_asset_path=face_model),
            num_faces=1,
            min_face_detection_confidence=0.5,
            min_face_presence_confidence=0.5,
            min_tracking_confidence=0.5,
            output_face_blendshapes=True,
        )
        self.face_landmarker = _MP_VIS.FaceLandmarker.create_from_options(face_opts)

        # PoseLandmarker ìƒì„±
        pose_opts = _MP_VIS.PoseLandmarkerOptions(
            base_options=_BASE_OPTS(model_asset_path=pose_model),
            num_poses=1,
            min_pose_detection_confidence=0.5,
            min_pose_presence_confidence=0.5,
            min_tracking_confidence=0.5,
        )
        self.pose_landmarker = _MP_VIS.PoseLandmarker.create_from_options(pose_opts)

    def _init_opencv_fallback(self):
        """OpenCV í´ë°± ì´ˆê¸°í™”"""
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )

    def analyze_frame(self, frame: np.ndarray, timestamp: float) -> VisionMetrics:
        """
        ë‹¨ì¼ í”„ë ˆì„ì˜ ë¹„ì–¸ì–´ì  ìš”ì†Œ ë¶„ì„

        Args:
            frame: BGR ì´ë¯¸ì§€ (OpenCV í˜•ì‹)
            timestamp: í”„ë ˆì„ íƒ€ì„ìŠ¤íƒ¬í”„ (ì´ˆ)

        Returns:
            VisionMetrics ê°ì²´
        """
        metrics = VisionMetrics(timestamp=timestamp)

        if self.use_mediapipe:
            self._analyze_mediapipe(frame, metrics)
        else:
            self._analyze_opencv(frame, metrics)

        # ê³µí†µ: ì›€ì§ì„ ë¶„ì„
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        self._analyze_motion(gray, metrics)

        self.prev_frame = frame.copy()
        self.prev_gray = gray.copy()

        self.results.append(metrics)
        return metrics

    # ================================================================
    # MediaPipe Tasks ë¶„ì„
    # ================================================================
    def _analyze_mediapipe(self, frame: np.ndarray, metrics: VisionMetrics):
        """MediaPipe Tasks API ê¸°ë°˜ ì¢…í•© ë¶„ì„"""
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb)
        h, w = frame.shape[:2]

        # ì–¼êµ´ ëœë“œë§ˆí¬ ë¶„ì„
        try:
            face_result = self.face_landmarker.detect(mp_image)
            if face_result.face_landmarks:
                metrics.face_detected = True
                lm = face_result.face_landmarks[0]  # ì²« ë²ˆì§¸ ì–¼êµ´
                self._analyze_gaze(lm, w, h, metrics)
                self._analyze_expression(face_result, metrics)
                self._analyze_head_tilt(lm, h, metrics)
        except Exception:
            pass

        # ìì„¸/ì œìŠ¤ì²˜ ë¶„ì„
        try:
            pose_result = self.pose_landmarker.detect(mp_image)
            if pose_result.pose_landmarks:
                pose_lm = pose_result.pose_landmarks[0]
                self._analyze_pose(pose_lm, w, h, metrics)
        except Exception:
            pass

    def _analyze_gaze(self, landmarks, w, h, metrics: VisionMetrics):
        """ëœë“œë§ˆí¬ ê¸°ë°˜ ì‹œì„  ë°©í–¥ ë¶„ì„"""
        # ì½” ë (landmark 1) ê¸°ì¤€ ì‹œì„  ì¶”ì •
        nose = landmarks[1]
        nose_x = nose.x

        # ì™¼ìª½ ëˆˆ (landmark 33), ì˜¤ë¥¸ìª½ ëˆˆ (landmark 263)
        left_eye = landmarks[33]
        right_eye = landmarks[263]

        eye_center_x = (left_eye.x + right_eye.x) / 2
        eye_center_y = (left_eye.y + right_eye.y) / 2

        # ì‹œì„  ë°©í–¥ íŒë³„
        x_offset = nose_x - eye_center_x
        if abs(x_offset) < 0.02:
            metrics.gaze_direction = "center"
            metrics.eye_contact = True
        elif x_offset > 0.02:
            metrics.gaze_direction = "right"
            metrics.eye_contact = False
        else:
            metrics.gaze_direction = "left"
            metrics.eye_contact = False

        # ì•½ê°„ì˜ ì¢Œìš° ì›€ì§ì„ë„ í•™ìƒ ì‹œì„  ì ‘ì´‰ìœ¼ë¡œ ê°„ì£¼ (Â±3ë„ ë²”ìœ„)
        if abs(x_offset) < 0.04 and abs(eye_center_y - 0.35) < 0.15:
            metrics.eye_contact = True

    def _analyze_expression(self, face_result, metrics: VisionMetrics):
        """Blendshape ê¸°ë°˜ í‘œì • ë¶„ì„"""
        if face_result.face_blendshapes:
            blendshapes = face_result.face_blendshapes[0]
            bs_dict = {bs.category_name: bs.score for bs in blendshapes}

            # ë¯¸ì†Œ ì •ë„
            smile = max(bs_dict.get('mouthSmileLeft', 0), bs_dict.get('mouthSmileRight', 0))
            # ëˆˆ í¬ê¸° (openness)
            eye_open = (bs_dict.get('eyeBlinkLeft', 0) + bs_dict.get('eyeBlinkRight', 0)) / 2
            # ì… ì—´ë¦¼ (ë§í•˜ê¸°)
            mouth_open = bs_dict.get('jawOpen', 0)
            # ëˆˆì¹ ì˜¬ë¦¼ (ê°•ì¡°)
            brow_up = max(bs_dict.get('browOuterUpLeft', 0), bs_dict.get('browOuterUpRight', 0))

            # í‘œì • ì ìˆ˜ (0-100)
            # ë¯¸ì†Œ + ë§í•˜ê¸° í™œë™ + ëˆˆì¹ í‘œí˜„ â†’ ë†’ì€ ì ìˆ˜
            expression = 50.0
            expression += smile * 30  # ë¯¸ì†Œ â†’ +30ê¹Œì§€
            expression += mouth_open * 15  # ë§í•˜ê¸° â†’ +15ê¹Œì§€
            expression += brow_up * 10  # ê°•ì¡° â†’ +10ê¹Œì§€
            expression -= eye_open * 20  # ëˆˆ ê°ê¸° â†’ -20ê¹Œì§€

            metrics.expression_score = max(0, min(100, expression))

    def _analyze_head_tilt(self, landmarks, h, metrics: VisionMetrics):
        """ë¨¸ë¦¬ ê¸°ìš¸ê¸° ë¶„ì„"""
        # ì™¼ìª½ ê·€ (127), ì˜¤ë¥¸ìª½ ê·€ (356)
        left_ear = landmarks[127]
        right_ear = landmarks[356]

        dy = (right_ear.y - left_ear.y) * h
        dx = (right_ear.x - left_ear.x) * h
        if dx != 0:
            metrics.head_tilt = np.degrees(np.arctan2(dy, dx))

    def _analyze_pose(self, pose_lm, w, h, metrics: VisionMetrics):
        """MediaPipe Pose ê¸°ë°˜ ì œìŠ¤ì²˜/ìì„¸ ë¶„ì„"""
        # ì£¼ìš” ëœë“œë§ˆí¬ ì¢Œí‘œ
        # 11: ì™¼ìª½ ì–´ê¹¨, 12: ì˜¤ë¥¸ìª½ ì–´ê¹¨
        # 13: ì™¼ìª½ íŒ”ê¿ˆì¹˜, 14: ì˜¤ë¥¸ìª½ íŒ”ê¿ˆì¹˜
        # 15: ì™¼ìª½ ì†ëª©, 16: ì˜¤ë¥¸ìª½ ì†ëª©
        # 23: ì™¼ìª½ ì—‰ë©ì´, 24: ì˜¤ë¥¸ìª½ ì—‰ë©ì´

        left_shoulder = pose_lm[11]
        right_shoulder = pose_lm[12]
        left_elbow = pose_lm[13]
        right_elbow = pose_lm[14]
        left_wrist = pose_lm[15]
        right_wrist = pose_lm[16]

        # 1. ì–´ê¹¨ ê°ë„ (body openness)
        shoulder_width = abs(right_shoulder.x - left_shoulder.x) * w
        shoulder_center_y = (left_shoulder.y + right_shoulder.y) / 2

        # 2. ì† ìœ„ì¹˜ íŒë³„
        hand_y_avg = (left_wrist.y + right_wrist.y) / 2
        shoulder_y = shoulder_center_y

        if hand_y_avg < shoulder_y - 0.1:
            metrics.hand_position = "above_shoulder"
            metrics.gesture_active = True
            metrics.gesture_score = 0.9
        elif hand_y_avg < shoulder_y + 0.05:
            metrics.hand_position = "shoulder_level"
            metrics.gesture_active = True
            metrics.gesture_score = 0.7
        elif hand_y_avg < shoulder_y + 0.2:
            metrics.hand_position = "chest_level"
            metrics.gesture_active = True
            metrics.gesture_score = 0.5
        else:
            metrics.hand_position = "waist_or_below"
            metrics.gesture_active = False
            metrics.gesture_score = 0.1

        # 3. ì–‘íŒ” ë²Œë¦¼ (body openness)
        left_arm_spread = abs(left_wrist.x - left_shoulder.x) * w
        right_arm_spread = abs(right_wrist.x - right_shoulder.x) * w
        arm_spread = (left_arm_spread + right_arm_spread) / 2

        if arm_spread > shoulder_width * 0.8:
            metrics.body_openness = 0.9
        elif arm_spread > shoulder_width * 0.5:
            metrics.body_openness = 0.7
        elif arm_spread > shoulder_width * 0.3:
            metrics.body_openness = 0.5
        else:
            metrics.body_openness = 0.3

        # 4. ì–´ê¹¨ ê¸°ìš¸ê¸°
        dy = (right_shoulder.y - left_shoulder.y) * h
        dx = (right_shoulder.x - left_shoulder.x) * w
        if dx != 0:
            metrics.shoulder_angle = np.degrees(np.arctan2(dy, dx))

        # 5. ì œìŠ¤ì²˜ ìœ í˜• íŒë³„
        left_elbow_angle = self._calc_angle(left_shoulder, left_elbow, left_wrist)
        right_elbow_angle = self._calc_angle(right_shoulder, right_elbow, right_wrist)

        if metrics.hand_position == "above_shoulder":
            metrics.arm_gesture_type = "pointing_up"
        elif left_elbow_angle < 90 or right_elbow_angle < 90:
            metrics.arm_gesture_type = "bent_active"
        elif left_elbow_angle > 150 and right_elbow_angle > 150:
            metrics.arm_gesture_type = "extended"
        else:
            metrics.arm_gesture_type = "relaxed"

    def _calc_angle(self, a, b, c):
        """ì„¸ í¬ì¸íŠ¸ë¡œ ê°ë„ ê³„ì‚°"""
        ba = np.array([a.x - b.x, a.y - b.y])
        bc = np.array([c.x - b.x, c.y - b.y])
        cos_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-8)
        return np.degrees(np.arccos(np.clip(cos_angle, -1.0, 1.0)))

    # ================================================================
    # OpenCV í´ë°± ë¶„ì„
    # ================================================================
    def _analyze_opencv(self, frame: np.ndarray, metrics: VisionMetrics):
        """OpenCV ê¸°ë°˜ ê°„ì†Œí™” ë¶„ì„ (MediaPipe ë¯¸ì„¤ì¹˜ í™˜ê²½)"""
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        h, w = gray.shape

        faces = self.face_cascade.detectMultiScale(gray, 1.3, 5)
        if len(faces) > 0:
            metrics.face_detected = True
            x, y, fw, fh = faces[0]
            face_center_x = x + fw / 2
            if abs(face_center_x - w / 2) < w * 0.15:
                metrics.eye_contact = True
                metrics.gaze_direction = "center"

            face_roi = gray[y:y+fh, x:x+fw]
            std = np.std(face_roi)
            metrics.expression_score = min(100, 40 + std * 0.8)

        self._analyze_regions_opencv(frame, metrics)

    def _analyze_regions_opencv(self, frame: np.ndarray, metrics: VisionMetrics):
        """í”„ë ˆì„ ì˜ì—­ë³„ ë¶„ì„ (í´ë°±)"""
        h, w = frame.shape[:2]

        upper = frame[:h//3, :]
        upper_gray = cv2.cvtColor(upper, cv2.COLOR_BGR2GRAY)
        upper_motion = np.std(upper_gray)

        if upper_motion > 40:
            metrics.gesture_active = True
            metrics.gesture_score = min(1.0, upper_motion / 80)
            metrics.hand_position = "above_shoulder"
        elif upper_motion > 25:
            metrics.gesture_active = True
            metrics.gesture_score = 0.4
            metrics.hand_position = "chest_level"

        # ì „ì²´ ë°ê¸° ë³€í™” â†’ ì²´í˜• ê°œë°©ë„ ì¶”ì •
        center = frame[h//4:3*h//4, w//4:3*w//4]
        metrics.body_openness = min(1.0, np.std(cv2.cvtColor(center, cv2.COLOR_BGR2GRAY)) / 60)

    # ================================================================
    # ê³µí†µ ë¶„ì„
    # ================================================================
    def _analyze_motion(self, gray: np.ndarray, metrics: VisionMetrics):
        """ì›€ì§ì„ ë¶„ì„"""
        if self.prev_gray is not None:
            diff = cv2.absdiff(gray, self.prev_gray)
            motion = np.mean(diff)
            metrics.motion_score = float(motion)
        else:
            metrics.motion_score = 0.0

    # ================================================================
    # ìš”ì•½ ë° íƒ€ì„ë¼ì¸
    # ================================================================
    def get_summary(self) -> Dict:
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½"""
        if not self.results:
            return {"error": "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}

        total = len(self.results)

        summary = {
            "total_frames_analyzed": total,
            "analysis_engine": "mediapipe_tasks" if self.use_mediapipe else "opencv",
            "gesture_active_ratio": sum(1 for r in self.results if r.gesture_active) / total,
            "avg_gesture_score": float(np.mean([r.gesture_score for r in self.results])),
            "eye_contact_ratio": sum(1 for r in self.results if r.eye_contact) / total,
            "face_detection_ratio": sum(1 for r in self.results if r.face_detected) / total,
            "avg_expression_score": float(np.mean([r.expression_score for r in self.results])),
            "avg_body_openness": float(np.mean([r.body_openness for r in self.results])),
            "avg_motion_score": float(np.mean([r.motion_score for r in self.results])),
            "hand_position_distribution": self._get_hand_distribution(),
        }

        # v6.0 ì¶”ê°€ ìš”ì•½
        if self.use_mediapipe:
            summary["gaze_distribution"] = self._get_gaze_distribution()
            summary["gesture_type_distribution"] = self._get_gesture_type_distribution()
            summary["avg_head_tilt"] = float(np.mean([abs(r.head_tilt) for r in self.results]))

        return summary

    def _get_hand_distribution(self) -> Dict:
        """ì† ìœ„ì¹˜ ë¶„í¬"""
        dist = {}
        total = len(self.results) or 1
        for r in self.results:
            dist[r.hand_position] = dist.get(r.hand_position, 0) + 1
        return {k: round(v / total, 3) for k, v in dist.items()}

    def _get_gaze_distribution(self) -> Dict:
        """ì‹œì„  ë°©í–¥ ë¶„í¬ (v6.0)"""
        dist = {}
        total = len(self.results) or 1
        for r in self.results:
            dist[r.gaze_direction] = dist.get(r.gaze_direction, 0) + 1
        return {k: round(v / total, 3) for k, v in dist.items()}

    def _get_gesture_type_distribution(self) -> Dict:
        """ì œìŠ¤ì²˜ ìœ í˜• ë¶„í¬ (v6.0)"""
        dist = {}
        total = len(self.results) or 1
        for r in self.results:
            dist[r.arm_gesture_type] = dist.get(r.arm_gesture_type, 0) + 1
        return {k: round(v / total, 3) for k, v in dist.items()}

    def get_timeline(self) -> List[Dict]:
        """ì‹œê°„ë³„ ë¶„ì„ ê²°ê³¼"""
        return [
            {
                "timestamp": r.timestamp,
                "gesture_active": r.gesture_active,
                "gesture_score": round(r.gesture_score, 2),
                "eye_contact": r.eye_contact,
                "expression_score": round(r.expression_score, 1),
                "body_openness": round(r.body_openness, 2),
                "motion_score": round(r.motion_score, 1),
                "gaze_direction": r.gaze_direction,
                "arm_gesture_type": r.arm_gesture_type,
            }
            for r in self.results
        ]

    def reset(self):
        """ë¶„ì„ ê²°ê³¼ ì´ˆê¸°í™”"""
        self.results.clear()
        self.prev_frame = None
        self.prev_gray = None
        self.prev_landmarks = None
