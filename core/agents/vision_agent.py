"""
ğŸ‘ï¸ Vision Agent - ê°•ì‚¬ ë¹„ì–¸ì–´ í–‰ë™ ë¶„ì„
OpenCV + ê°„ì†Œí™”ëœ ë¶„ì„ (MediaPipe í˜¸í™˜ì„± ì´ìŠˆë¡œ ì¸í•´)
"""

import cv2
import numpy as np
from dataclasses import dataclass
from typing import List, Dict, Optional
from pathlib import Path


@dataclass
class VisionMetrics:
    """ë¹„ì „ ë¶„ì„ ê²°ê³¼ ë©”íŠ¸ë¦­"""
    timestamp: float
    gesture_active: bool = False           # ì œìŠ¤ì²˜ í™œì„±í™” ì—¬ë¶€
    gesture_score: float = 0.0             # ì œìŠ¤ì²˜ ì—­ë™ì„± ì ìˆ˜ (0-100)
    hand_position: str = "unknown"         # low, mid, high
    eye_contact: bool = False              # ì •ë©´ ì‘ì‹œ ì—¬ë¶€
    face_detected: bool = False            # ì–¼êµ´ ê°ì§€ ì—¬ë¶€
    expression_score: float = 50.0         # í‘œì • í™œë ¥ ì ìˆ˜ (0-100)
    body_openness: float = 0.5             # ìì„¸ ê°œë°©ì„± (0-1)
    motion_score: float = 0.0              # ì›€ì§ì„ ì ìˆ˜


class VisionAgent:
    """
    ğŸ‘ï¸ Vision Agent
    ê°•ì‚¬ì˜ ë¹„ì–¸ì–´ì  ìš”ì†Œ(ì œìŠ¤ì²˜, ì‹œì„ , í‘œì •)ë¥¼ ë¶„ì„
    OpenCV ê¸°ë°˜ ê°„ì†Œí™” ë²„ì „ (MediaPipe ì˜ì¡´ì„± ì œê±°)
    """
    
    def __init__(self, config: Optional[dict] = None):
        self.config = config or {
            "gesture_threshold": 0.6,
            "face_confidence": 0.5
        }
        
        # OpenCV ì–¼êµ´ ê°ì§€ê¸°
        self.face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        
        # ì›€ì§ì„ ê°ì§€ë¥¼ ìœ„í•œ ì´ì „ í”„ë ˆì„
        self.prev_frame = None
        self.prev_gray = None
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.results: List[VisionMetrics] = []
    
    def analyze_frame(self, frame: np.ndarray, timestamp: float) -> VisionMetrics:
        """
        ë‹¨ì¼ í”„ë ˆì„ì˜ ë¹„ì–¸ì–´ì  ìš”ì†Œ ë¶„ì„
        
        Args:
            frame: BGR ì´ë¯¸ì§€ (OpenCV í˜•ì‹)
            timestamp: í”„ë ˆì„ íƒ€ì„ìŠ¤íƒ¬í”„ (ì´ˆ)
            
        Returns:
            VisionMetrics ê°ì²´
        """
        metrics = VisionMetrics(timestamp=timestamp)
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        
        # 1. ì–¼êµ´ ê°ì§€
        self._analyze_face(frame, gray, metrics)
        
        # 2. ì›€ì§ì„ ë¶„ì„ (ì œìŠ¤ì²˜ ëŒ€ìš©)
        self._analyze_motion(gray, metrics)
        
        # 3. í”„ë ˆì„ ì˜ì—­ë³„ í™œë™ì„± ë¶„ì„
        self._analyze_regions(frame, metrics)
        
        # ì´ì „ í”„ë ˆì„ ì €ì¥
        self.prev_frame = frame.copy()
        self.prev_gray = gray.copy()
        
        self.results.append(metrics)
        return metrics
    
    def _analyze_face(self, frame: np.ndarray, gray: np.ndarray, metrics: VisionMetrics):
        """ì–¼êµ´ ê°ì§€ ë° ë¶„ì„"""
        height, width = frame.shape[:2]
        
        faces = self.face_cascade.detectMultiScale(
            gray, 
            scaleFactor=1.1, 
            minNeighbors=4,
            minSize=(30, 30)
        )
        
        if len(faces) > 0:
            metrics.face_detected = True
            
            # ê°€ì¥ í° ì–¼êµ´ ì„ íƒ
            largest_face = max(faces, key=lambda f: f[2] * f[3])
            x, y, w, h = largest_face
            
            # ì–¼êµ´ ì¤‘ì‹¬ ìœ„ì¹˜ë¡œ ì‹œì„  ì¶”ì •
            face_center_x = (x + w // 2) / width
            face_center_y = (y + h // 2) / height
            
            # ì¤‘ì•™ì— ê°€ê¹Œìš°ë©´ ì •ë©´ ì‘ì‹œë¡œ íŒë‹¨
            is_centered = 0.3 < face_center_x < 0.7
            metrics.eye_contact = is_centered
            
            # ì–¼êµ´ í¬ê¸°ë¡œ í‘œì • í™œë ¥ ì¶”ì •
            # (ì–¼êµ´ì´ í¬ê²Œ ë³´ì´ë©´ ì¹´ë©”ë¼ì— ê°€ê¹Œì´ = ë” ì ê·¹ì )
            face_size_ratio = (w * h) / (width * height)
            metrics.expression_score = min(100, face_size_ratio * 1000)
    
    def _analyze_motion(self, gray: np.ndarray, metrics: VisionMetrics):
        """ì›€ì§ì„ ë¶„ì„ (ì œìŠ¤ì²˜ ëŒ€ìš©)"""
        if self.prev_gray is None:
            return
        
        # í”„ë ˆì„ ì°¨ì´ ê³„ì‚°
        diff = cv2.absdiff(gray, self.prev_gray)
        _, thresh = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)
        
        # ì›€ì§ì„ ë¹„ìœ¨ ê³„ì‚°
        motion_pixels = np.count_nonzero(thresh)
        total_pixels = thresh.size
        motion_ratio = motion_pixels / total_pixels
        
        # ì›€ì§ì„ ì ìˆ˜ (0-100)
        metrics.motion_score = min(100, motion_ratio * 500)
        
        # ì›€ì§ì„ì´ ì¶©ë¶„í•˜ë©´ ì œìŠ¤ì²˜ í™œì„±í™”ë¡œ íŒë‹¨
        metrics.gesture_active = motion_ratio > 0.02
        metrics.gesture_score = metrics.motion_score
    
    def _analyze_regions(self, frame: np.ndarray, metrics: VisionMetrics):
        """í”„ë ˆì„ ì˜ì—­ë³„ ë¶„ì„"""
        height, width = frame.shape[:2]
        
        # ìƒí•˜ ì˜ì—­ ë¶„í• 
        top_half = frame[:height//2, :]
        bottom_half = frame[height//2:, :]
        
        # ê° ì˜ì—­ì˜ í™œë™ì„± (ì—£ì§€ ë°€ë„)
        top_edges = cv2.Canny(cv2.cvtColor(top_half, cv2.COLOR_BGR2GRAY), 50, 150)
        bottom_edges = cv2.Canny(cv2.cvtColor(bottom_half, cv2.COLOR_BGR2GRAY), 50, 150)
        
        top_activity = np.count_nonzero(top_edges) / top_edges.size
        bottom_activity = np.count_nonzero(bottom_edges) / bottom_edges.size
        
        # ìƒë‹¨ í™œë™ì´ ë§ìœ¼ë©´ ì†ì´ ìœ„ì— ìˆëŠ” ê²ƒìœ¼ë¡œ ì¶”ì •
        if top_activity > bottom_activity * 1.5:
            metrics.hand_position = "high"
        elif bottom_activity > top_activity * 1.5:
            metrics.hand_position = "low"
        else:
            metrics.hand_position = "mid"
        
        # ìì„¸ ê°œë°©ì„± ì¶”ì •
        # í”„ë ˆì„ ì „ì²´ì— í™œë™ì´ ê³ ë¥´ê²Œ ë¶„í¬í•˜ë©´ ê°œë°©ì 
        metrics.body_openness = min(1.0, (top_activity + bottom_activity) * 10)
    
    def get_summary(self) -> Dict:
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½"""
        if not self.results:
            return {"error": "ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        total = len(self.results)
        
        return {
            "total_frames_analyzed": total,
            "gesture_active_ratio": sum(1 for r in self.results if r.gesture_active) / total,
            "avg_gesture_score": np.mean([r.gesture_score for r in self.results]),
            "eye_contact_ratio": sum(1 for r in self.results if r.eye_contact) / total,
            "face_detection_ratio": sum(1 for r in self.results if r.face_detected) / total,
            "avg_expression_score": np.mean([r.expression_score for r in self.results]),
            "avg_body_openness": np.mean([r.body_openness for r in self.results]),
            "avg_motion_score": np.mean([r.motion_score for r in self.results]),
            "hand_position_distribution": self._get_hand_distribution()
        }
    
    def _get_hand_distribution(self) -> Dict[str, float]:
        """ì† ìœ„ì¹˜ ë¶„í¬"""
        if not self.results:
            return {}
        
        positions = [r.hand_position for r in self.results]
        total = len(positions)
        
        return {
            "high": positions.count("high") / total,
            "mid": positions.count("mid") / total,
            "low": positions.count("low") / total
        }
    
    def get_timeline(self) -> List[Dict]:
        """ì‹œê°„ë³„ ë¶„ì„ ê²°ê³¼"""
        return [
            {
                "timestamp": r.timestamp,
                "gesture_score": r.gesture_score,
                "eye_contact": r.eye_contact,
                "expression_score": r.expression_score,
                "motion_score": r.motion_score
            }
            for r in self.results
        ]
    
    def reset(self):
        """ë¶„ì„ ê²°ê³¼ ì´ˆê¸°í™”"""
        self.results = []
        self.prev_frame = None
        self.prev_gray = None
