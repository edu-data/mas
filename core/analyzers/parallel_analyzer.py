"""
âš¡ Parallel Frame Analyzer - ë©€í‹°í”„ë¡œì„¸ì‹± í”„ë ˆì„ ë¶„ì„
CPU ì½”ì–´ë¥¼ í™œìš©í•œ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë¶„ì„ ì‹œê°„ ë‹¨ì¶•
"""

import cv2
import numpy as np
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
from multiprocessing import cpu_count
import warnings

warnings.filterwarnings("ignore")


@dataclass
class FrameResult:
    """í”„ë ˆì„ ë¶„ì„ ê²°ê³¼"""
    timestamp: float
    vision_metrics: Dict
    content_metrics: Dict


def analyze_single_frame(args: Tuple) -> Dict:
    """
    ë‹¨ì¼ í”„ë ˆì„ ë¶„ì„ (í”„ë¡œì„¸ìŠ¤ ì›Œì»¤ì—ì„œ ì‹¤í–‰)
    
    Args:
        args: (frame_bytes, timestamp, frame_shape, config)
    """
    frame_bytes, timestamp, frame_shape, config = args
    
    # ë°”ì´íŠ¸ë¥¼ numpy arrayë¡œ ë³µì›
    frame = np.frombuffer(frame_bytes, dtype=np.uint8).reshape(frame_shape)
    
    result = {
        "timestamp": timestamp,
        "vision": {},
        "content": {}
    }
    
    try:
        # Vision ë¶„ì„
        result["vision"] = _analyze_vision(frame, timestamp, config.get("vision", {}))
        
        # Content ë¶„ì„
        result["content"] = _analyze_content(frame, timestamp, config.get("content", {}))
        
    except Exception as e:
        result["error"] = str(e)
    
    return result


def _analyze_vision(frame: np.ndarray, timestamp: float, config: Dict) -> Dict:
    """Vision ë¶„ì„ (ì›Œì»¤ ë‚´ë¶€)"""
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    height, width = frame.shape[:2]
    
    # ì–¼êµ´ ê°ì§€
    face_cascade = cv2.CascadeClassifier(
        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
    )
    faces = face_cascade.detectMultiScale(gray, 1.1, 4, minSize=(30, 30))
    
    face_detected = len(faces) > 0
    eye_contact = False
    expression_score = 50.0
    
    if face_detected:
        x, y, w, h = max(faces, key=lambda f: f[2] * f[3])
        face_center_x = (x + w // 2) / width
        eye_contact = 0.3 < face_center_x < 0.7
        face_size_ratio = (w * h) / (width * height)
        expression_score = min(100, face_size_ratio * 1000)
    
    # ì›€ì§ì„ ë¶„ì„ì€ ë‹¨ì¼ í”„ë ˆì„ì—ì„œ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ ê¸°ë³¸ê°’
    return {
        "timestamp": timestamp,
        "face_detected": face_detected,
        "eye_contact": eye_contact,
        "expression_score": expression_score,
        "gesture_score": 0.0,
        "motion_score": 0.0
    }


def _analyze_content(frame: np.ndarray, timestamp: float, config: Dict) -> Dict:
    """Content ë¶„ì„ (ì›Œì»¤ ë‚´ë¶€)"""
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    height, width = frame.shape[:2]
    
    # ë°ê¸°
    brightness = float(np.mean(gray))
    
    # ìƒ‰ìƒ ëŒ€ë¹„
    lab = cv2.cvtColor(frame, cv2.COLOR_BGR2LAB)
    l_channel = lab[:, :, 0]
    color_contrast = float(np.std(l_channel) / 128)
    
    # í…ìŠ¤íŠ¸ ë°€ë„ ì¶”ì • (MSER)
    mser = cv2.MSER_create()
    regions, _ = mser.detectRegions(gray)
    estimated_chars = len(regions) // 3
    
    threshold = config.get("text_density_threshold", 150)
    density_ratio = estimated_chars / threshold
    text_density_score = min(10, max(1, int(density_ratio * 5) + 1))
    
    # ë³µì¡ë„
    laplacian = cv2.Laplacian(gray, cv2.CV_64F)
    complexity_score = min(100, float(laplacian.var()) / 50)
    
    # ìŠ¬ë¼ì´ë“œ ê°ì§€
    center = frame[height//4:3*height//4, width//4:3*width//4]
    center_gray = cv2.cvtColor(center, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(center_gray, 50, 150)
    edge_density = np.sum(edges > 0) / edges.size
    slide_detected = 0.01 < edge_density < 0.3
    
    return {
        "timestamp": timestamp,
        "text_density": estimated_chars,
        "text_density_score": text_density_score,
        "brightness": brightness,
        "complexity_score": complexity_score,
        "slide_detected": slide_detected,
        "speaker_visible": False,
        "speaker_overlap": False
    }


class ParallelFrameAnalyzer:
    """
    âš¡ ë³‘ë ¬ í”„ë ˆì„ ë¶„ì„ê¸°
    
    CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ í”„ë¡œì„¸ìŠ¤ í’€ì„ ìƒì„±í•˜ê³ 
    í”„ë ˆì„ë“¤ì„ ë³‘ë ¬ë¡œ ë¶„ì„
    """
    
    def __init__(self, max_workers: int = None, config: Dict = None):
        """
        Args:
            max_workers: ì›Œì»¤ ìˆ˜ (Noneì´ë©´ CPU ì½”ì–´ ìˆ˜ - 1)
            config: Vision/Content ì„¤ì •
        """
        self.max_workers = max_workers or max(1, cpu_count() - 1)
        self.config = config or {}
        
        print(f"âš¡ ë³‘ë ¬ ë¶„ì„ê¸° ì´ˆê¸°í™” (ì›Œì»¤: {self.max_workers}ê°œ)")
    
    def analyze_frames(
        self,
        frames: List[Tuple[np.ndarray, float]],
        show_progress: bool = True
    ) -> Tuple[List[Dict], List[Dict]]:
        """
        í”„ë ˆì„ ë¦¬ìŠ¤íŠ¸ ë³‘ë ¬ ë¶„ì„
        
        Args:
            frames: [(frame, timestamp), ...] ë¦¬ìŠ¤íŠ¸
            show_progress: ì§„í–‰ë¥  í‘œì‹œ ì—¬ë¶€
            
        Returns:
            (vision_results, content_results) íŠœí”Œ
        """
        if not frames:
            return [], []
        
        total = len(frames)
        print(f"âš¡ ë³‘ë ¬ ë¶„ì„ ì‹œì‘: {total}ê°œ í”„ë ˆì„, {self.max_workers}ê°œ ì›Œì»¤")
        
        # í”„ë ˆì„ì„ ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
        tasks = []
        for frame, timestamp in frames:
            frame_bytes = frame.tobytes()
            frame_shape = frame.shape
            tasks.append((frame_bytes, timestamp, frame_shape, self.config))
        
        vision_results = []
        content_results = []
        
        # ë³‘ë ¬ ì²˜ë¦¬
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(analyze_single_frame, task): i 
                for i, task in enumerate(tasks)
            }
            
            completed = 0
            for future in as_completed(futures):
                completed += 1
                
                if show_progress and completed % 100 == 0:
                    pct = (completed / total) * 100
                    print(f"   âš¡ ì§„í–‰: {completed}/{total} ({pct:.1f}%)")
                
                try:
                    result = future.result()
                    vision_results.append(result["vision"])
                    content_results.append(result["content"])
                except Exception as e:
                    print(f"   [!] í”„ë ˆì„ ë¶„ì„ ì˜¤ë¥˜: {e}")
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ìˆœìœ¼ë¡œ ì •ë ¬
        vision_results.sort(key=lambda x: x.get("timestamp", 0))
        content_results.sort(key=lambda x: x.get("timestamp", 0))
        
        print(f"âœ… ë³‘ë ¬ ë¶„ì„ ì™„ë£Œ: {len(vision_results)}ê°œ")
        
        return vision_results, content_results
    
    def analyze_video(
        self,
        video_path: Path,
        sample_rate: float = 1.0
    ) -> Tuple[List[Dict], List[Dict]]:
        """
        ë¹„ë””ì˜¤ íŒŒì¼ ì§ì ‘ ë¶„ì„
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            sample_rate: ì´ˆë‹¹ ìƒ˜í”Œë§ í”„ë ˆì„ ìˆ˜
            
        Returns:
            (vision_results, content_results)
        """
        video_path = Path(video_path)
        cap = cv2.VideoCapture(str(video_path))
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps
        
        frame_interval = int(fps / sample_rate)
        
        print(f"ğŸ¬ ë¹„ë””ì˜¤ ë¡œë“œ: {video_path.name}")
        print(f"   FPS: {fps:.1f}, ê¸¸ì´: {duration:.1f}ì´ˆ, ìƒ˜í”Œë§: {sample_rate}/ì´ˆ")
        
        # ë¨¼ì € ëª¨ë“  í”„ë ˆì„ ì¶”ì¶œ (ë©”ëª¨ë¦¬ì— ë¡œë“œ)
        frames = []
        frame_idx = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            if frame_idx % frame_interval == 0:
                timestamp = frame_idx / fps
                frames.append((frame.copy(), timestamp))
            
            frame_idx += 1
        
        cap.release()
        print(f"   í”„ë ˆì„ ì¶”ì¶œ ì™„ë£Œ: {len(frames)}ê°œ")
        
        # ë³‘ë ¬ ë¶„ì„
        return self.analyze_frames(frames)


# í¸ì˜ í•¨ìˆ˜
def parallel_analyze(
    video_path: Path,
    sample_rate: float = 1.0,
    max_workers: int = None,
    config: Dict = None
) -> Tuple[List[Dict], List[Dict]]:
    """
    ë¹„ë””ì˜¤ ë³‘ë ¬ ë¶„ì„ í¸ì˜ í•¨ìˆ˜
    
    Args:
        video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        sample_rate: ì´ˆë‹¹ í”„ë ˆì„ ìƒ˜í”Œë§
        max_workers: ì›Œì»¤ ìˆ˜
        config: ì„¤ì •
        
    Returns:
        (vision_results, content_results)
    """
    analyzer = ParallelFrameAnalyzer(max_workers=max_workers, config=config)
    return analyzer.analyze_video(video_path, sample_rate)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    import time
    
    video = Path(r"D:\data science\02.21\ë…¹í™”_2025_02_21_08_37_50_910.mp4")
    
    start = time.time()
    vision, content = parallel_analyze(video, sample_rate=1.0)
    elapsed = time.time() - start
    
    print(f"\në¶„ì„ ì‹œê°„: {elapsed:.1f}ì´ˆ")
    print(f"Vision ê²°ê³¼: {len(vision)}ê°œ")
    print(f"Content ê²°ê³¼: {len(content)}ê°œ")
