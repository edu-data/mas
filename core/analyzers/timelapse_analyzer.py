"""
âš¡ Time-Lapse Analyzer - ì´ˆê³ ì† íƒ€ì„ë©ìŠ¤ ë¶„ì„ ëª¨ë“ˆ
FFmpeg + Multiprocessingìœ¼ë¡œ 15ë¶„ ì˜ìƒì„ 60ì´ˆ ë‚´ì— ë¶„ì„

í•µì‹¬ ì „ëµ:
1. FFmpeg C ë ˆë²¨ ë””ì½”ë”©ìœ¼ë¡œ I/O ë³‘ëª© ì œê±°
2. MediaPipe Lite (model_complexity=0) ì‚¬ìš©
3. Vision + Audio ì™„ì „ ë³‘ë ¬ ì²˜ë¦¬
"""

import os
import glob
import shutil
import subprocess
import multiprocessing
import time
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass

import cv2
import numpy as np

# MediaPipe ì¡°ê±´ë¶€ ì„í¬íŠ¸
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False
    print("[!] MediaPipe not available, using OpenCV fallback")

# Librosa ì¡°ê±´ë¶€ ì„í¬íŠ¸
try:
    import librosa
    LIBROSA_AVAILABLE = True
except ImportError:
    LIBROSA_AVAILABLE = False
    print("[!] Librosa not available, audio analysis disabled")

# pytesseract OCR ì„¤ì •
try:
    import pytesseract
    pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
    PYTESSERACT_AVAILABLE = True
except ImportError:
    PYTESSERACT_AVAILABLE = False
    print("[!] pytesseract not available, text extraction disabled")


@dataclass
class TurboAnalysisResult:
    """í„°ë³´ ë¶„ì„ ê²°ê³¼ ì»¨í…Œì´ë„ˆ"""
    timeline: List[Dict]
    audio_metrics: Dict
    audio_timeline: List[Dict]  # ì„¸ê·¸ë¨¼íŠ¸ë³„ ì˜¤ë””ì˜¤ íƒ€ì„ë¼ì¸
    elapsed_seconds: float
    frame_count: int


# ---------------------------------------------------------
# 1. [I/O Phase] FFmpegë¥¼ ì´ìš©í•œ ì´ˆê³ ì† ë¦¬ì†ŒìŠ¤ ì¶”ì¶œ
# ---------------------------------------------------------
def flash_extract_resources(video_path: str, output_dir: str, use_gpu: bool = True) -> Tuple[List[str], str]:
    """
    ì˜ìƒì—ì„œ 'ë¶„ì„ì— í•„ìš”í•œ ìµœì†Œí•œì˜ ë°ì´í„°'ë§Œ ë¬¼ë¦¬ì ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        video_path: ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (ì„ì‹œ ìºì‹œ)
        use_gpu: GPU ê°€ì† ì‚¬ìš© ì—¬ë¶€ (NVIDIA CUDA)
        
    Returns:
        (ì´ë¯¸ì§€ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸, ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ)
        
    Notes:
        - Video: 1ì´ˆì— 1ì¥(1fps), 360p í•´ìƒë„ë¡œ ì´ë¯¸ì§€ ì €ì¥
        - Audio: 16kHz ëª¨ë…¸ WAV íŒŒì¼ë¡œ ë¶„ë¦¬
    """
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)
    os.makedirs(output_dir)

    # GPU ê°€ì† ê°ì§€
    gpu_available = False
    if use_gpu:
        try:
            result = subprocess.run(
                ['ffmpeg', '-hwaccels'],
                capture_output=True, text=True, timeout=5
            )
            gpu_available = 'cuda' in result.stdout.lower()
        except Exception:
            pass
    
    accel_mode = "GPU (CUDA)" if gpu_available else "CPU"
    print(f"âš¡ [Phase 1] FFmpeg ë¦¬ì†ŒìŠ¤ ì¶”ì¶œ ì‹œì‘... [{accel_mode}]")
    
    # ë¹„ë””ì˜¤ ì¶”ì¶œ ëª…ë ¹ (GPU ê°€ì† ì ìš©)
    if gpu_available:
        # NVIDIA GPU: scale_cudaë¡œ GPUì—ì„œ ìŠ¤ì¼€ì¼ë§ê¹Œì§€ ì²˜ë¦¬
        cmd_vid = [
            'ffmpeg',
            '-hwaccel', 'cuda',
            '-hwaccel_output_format', 'cuda',
            '-i', video_path,
            '-vf', 'scale_cuda=640:360,hwdownload,format=nv12,fps=1',
            '-q:v', '2',
            f'{output_dir}/frame_%04d.jpg',
            '-loglevel', 'error', '-y'
        ]
    else:
        cmd_vid = [
            'ffmpeg', '-i', video_path,
            '-vf', 'fps=1,scale=640:360',
            '-q:v', '2',
            f'{output_dir}/frame_%04d.jpg',
            '-loglevel', 'error', '-y'
        ]
    
    # ì˜¤ë””ì˜¤ ì¶”ì¶œ ëª…ë ¹
    audio_path = os.path.join(output_dir, "audio.wav")
    cmd_aud = [
        'ffmpeg', '-i', video_path,
        '-ar', '16000', '-ac', '1',
        audio_path,
        '-loglevel', 'error', '-y'
    ]
    
    # ë‘ ì‘ì—…ì„ ë™ì‹œì— ë˜ì ¸ë†“ê³  ê¸°ë‹¤ë¦¼ (Parallel I/O)
    p1 = subprocess.Popen(cmd_vid, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    p2 = subprocess.Popen(cmd_aud, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    
    # GPU ëª…ë ¹ ì‹¤íŒ¨ ì‹œ CPU fallback
    exit_code = p1.wait()
    if exit_code != 0 and gpu_available:
        print("   [!] GPU ì¶”ì¶œ ì‹¤íŒ¨, CPU fallback...")
        cmd_vid_cpu = [
            'ffmpeg', '-i', video_path,
            '-vf', 'fps=1,scale=640:360',
            '-q:v', '2',
            f'{output_dir}/frame_%04d.jpg',
            '-loglevel', 'error', '-y'
        ]
        subprocess.run(cmd_vid_cpu, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    
    p2.wait()
    
    images = sorted(glob.glob(f'{output_dir}/*.jpg'))
    print(f"   âœ… ì¶”ì¶œ ì™„ë£Œ: ì´ë¯¸ì§€ {len(images)}ì¥, ì˜¤ë””ì˜¤ 1ê°œ")
    
    return images, audio_path


# ---------------------------------------------------------
# 2. [Vision Worker] ì´ë¯¸ì§€ ë°°ì¹˜ ë¶„ì„
# ---------------------------------------------------------
def analyze_vision_batch(image_paths: List[str]) -> List[Dict]:
    """
    í• ë‹¹ë°›ì€ ì´ë¯¸ì§€ ë¬¶ìŒ(Chunk)ì„ ì—°ì† ì²˜ë¦¬
    
    í•µì‹¬: í”„ë¡œì„¸ìŠ¤ë‹¹ ëª¨ë¸ì„ ë‹¨ í•œ ë²ˆë§Œ ë¡œë“œ(Load Once)
    
    Args:
        image_paths: ë¶„ì„í•  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    results = []
    
    # MediaPipe ì‚¬ìš© ê°€ëŠ¥ ì‹œ Pose ëª¨ë¸ ì´ˆê¸°í™”
    mp_pose = None
    if MEDIAPIPE_AVAILABLE:
        try:
            mp_pose = mp.solutions.pose.Pose(
                static_image_mode=True,
                model_complexity=0,  # Lite ëª¨ë¸ (ìµœê³  ì†ë„)
                min_detection_confidence=0.5
            )
        except Exception:
            mp_pose = None
    
    # OpenCV Haar Cascade (fallback)
    face_cascade = cv2.CascadeClassifier(
        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
    )
    
    for img_path in image_paths:
        try:
            # í”„ë ˆì„ ì¸ë±ìŠ¤ ì¶”ì¶œ (frame_0001.jpg -> 1)
            frame_idx = int(os.path.basename(img_path).split('_')[1].split('.')[0])
            
            frame = cv2.imread(img_path)
            if frame is None:
                continue
            
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            height, width = frame.shape[:2]
            
            # A. ì œìŠ¤ì²˜ ì—­ë™ì„± ë¶„ì„
            gesture_active = False
            face_detected = False
            eye_contact = False
            
            if mp_pose is not None:
                pose_res = mp_pose.process(frame_rgb)
                if pose_res.pose_landmarks:
                    # ì†ëª©(15,16)ì´ ê°€ìŠ´(0.7) ë†’ì´ë³´ë‹¤ ìœ„ì¸ì§€ í™•ì¸
                    lw_y = pose_res.pose_landmarks.landmark[15].y
                    rw_y = pose_res.pose_landmarks.landmark[16].y
                    if lw_y < 0.7 or rw_y < 0.7:
                        gesture_active = True
                    # ì½”(0) ìœ„ì¹˜ë¡œ ì–¼êµ´ ì¤‘ì•™ ì²´í¬
                    nose_x = pose_res.pose_landmarks.landmark[0].x
                    if 0.3 < nose_x < 0.7:
                        eye_contact = True
                    face_detected = True
            else:
                # Haar Cascade fallback
                faces = face_cascade.detectMultiScale(gray, 1.1, 4, minSize=(30, 30))
                if len(faces) > 0:
                    face_detected = True
                    x, y, w, h = max(faces, key=lambda f: f[2] * f[3])
                    face_center_x = (x + w // 2) / width
                    eye_contact = 0.3 < face_center_x < 0.7
            
            # B. ìŠ¬ë¼ì´ë“œ ë³µì¡ë„ (Canny Edge ë°€ë„)
            edges = cv2.Canny(gray, 100, 200)
            complexity = float(np.sum(edges)) / edges.size
            
            # C. í…ìŠ¤íŠ¸ ë°€ë„ ì¶”ì • (MSER)
            mser = cv2.MSER_create()
            regions, _ = mser.detectRegions(gray)
            text_density = len(regions) // 3
            
            results.append({
                "sec": frame_idx,
                "timestamp": float(frame_idx),
                "gesture_active": gesture_active,
                "face_detected": face_detected,
                "eye_contact": eye_contact,
                "slide_complexity": complexity,
                "text_density": text_density,
                "gesture_score": 1.0 if gesture_active else 0.0,
                "expression_score": 50.0 if face_detected else 0.0
            })
            
        except Exception as e:
            # ì˜¤ë¥˜ ì‹œ í•´ë‹¹ í”„ë ˆì„ ìŠ¤í‚µ
            continue
    
    # MediaPipe ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    if mp_pose is not None:
        mp_pose.close()
    
    return results


def analyze_audio_track(audio_path: str, segment_duration: float = 10.0) -> Tuple[Dict, List[Dict]]:
    """
    ì˜¤ë””ì˜¤ íŠ¸ë™ ë¶„ì„ (ì¹¨ë¬µ ë¹„ìœ¨, í”¼ì¹˜ ë‹¤ì–‘ì„±) + ì„¸ê·¸ë¨¼íŠ¸ë³„ íƒ€ì„ë¼ì¸
    
    Args:
        audio_path: WAV íŒŒì¼ ê²½ë¡œ
        segment_duration: ì„¸ê·¸ë¨¼íŠ¸ ê¸¸ì´ (ì´ˆ)
        
    Returns:
        (ì „ì²´ ì˜¤ë””ì˜¤ ë©”íŠ¸ë¦­, ì„¸ê·¸ë¨¼íŠ¸ë³„ íƒ€ì„ë¼ì¸ ë¦¬ìŠ¤íŠ¸)
    """
    empty_metrics = {
        "silence_ratio": 0.0,
        "pitch_std": 0.0,
        "energy_mean": 0.0,
        "is_monotone": False
    }
    
    if not LIBROSA_AVAILABLE:
        return empty_metrics, []
    
    try:
        # WAV íŒŒì¼ ì§ì ‘ ë¡œë”©
        y, sr = librosa.load(audio_path, sr=16000)
        total_duration = len(y) / sr
        
        # === ì „ì²´ ë©”íŠ¸ë¦­ ê³„ì‚° ===
        non_silent = librosa.effects.split(y, top_db=20)
        if len(y) > 0:
            speech_samples = sum(end - start for start, end in non_silent)
            silence_ratio = 1 - (speech_samples / len(y))
        else:
            silence_ratio = 1.0
        
        energy = librosa.feature.rms(y=y)[0]
        energy_mean = float(np.mean(energy))
        
        # í”¼ì¹˜ ë‹¤ì–‘ì„± (ì•ë¶€ë¶„ 60ì´ˆ ìƒ˜í”Œë§)
        sample_length = min(60 * sr, len(y))
        f0 = librosa.yin(y[:sample_length], fmin=50, fmax=300)
        pitch_std = float(np.std(f0[f0 > 0])) if np.any(f0 > 0) else 0.0
        is_monotone = pitch_std < 20.0
        
        overall_metrics = {
            "silence_ratio": silence_ratio,
            "pitch_std": pitch_std,
            "energy_mean": energy_mean,
            "is_monotone": is_monotone
        }
        
        # === ì„¸ê·¸ë¨¼íŠ¸ë³„ íƒ€ì„ë¼ì¸ ìƒì„± (ë³‘ë ¬í™” - ThreadPool) ===
        segment_samples = int(segment_duration * sr)
        segment_data = []
        
        # ì„¸ê·¸ë¨¼íŠ¸ ë°ì´í„° ì¤€ë¹„
        for seg_idx in range(0, len(y), segment_samples):
            seg_start = seg_idx
            seg_end = min(seg_idx + segment_samples, len(y))
            segment = y[seg_start:seg_end]
            
            if len(segment) < sr:  # 1ì´ˆ ë¯¸ë§Œ ì„¸ê·¸ë¨¼íŠ¸ ìŠ¤í‚µ
                continue
            
            segment_data.append((segment, seg_idx / sr, sr))
        
        # ThreadPoolExecutor ì‚¬ìš© (daemonic process ë¬¸ì œ íšŒí”¼)
        from concurrent.futures import ThreadPoolExecutor
        num_workers = max(2, multiprocessing.cpu_count() // 2)
        
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            segment_timeline = list(executor.map(_analyze_audio_segment, segment_data))
        
        return overall_metrics, segment_timeline
        
    except Exception as e:
        print(f"   [!] ì˜¤ë””ì˜¤ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return empty_metrics, []


def _analyze_audio_segment(args: Tuple) -> Dict:
    """
    ë‹¨ì¼ ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ (ë³‘ë ¬ ì›Œì»¤)
    
    Args:
        args: (segment_array, timestamp, sample_rate) íŠœí”Œ
    """
    segment, timestamp, sr = args
    
    try:
        # ì„¸ê·¸ë¨¼íŠ¸ ì—ë„ˆì§€
        seg_energy = librosa.feature.rms(y=segment)[0]
        seg_energy_mean = float(np.mean(seg_energy))
        
        # ì„¸ê·¸ë¨¼íŠ¸ ì¹¨ë¬µ ê°ì§€
        seg_non_silent = librosa.effects.split(segment, top_db=25)
        seg_speech = sum(end - start for start, end in seg_non_silent)
        seg_is_silent = seg_speech < len(segment) * 0.3
        
        # ì„¸ê·¸ë¨¼íŠ¸ í”¼ì¹˜
        seg_f0 = librosa.yin(segment, fmin=50, fmax=300)
        seg_pitch_std = float(np.std(seg_f0[seg_f0 > 0])) if np.any(seg_f0 > 0) else 0.0
        seg_pitch_mean = float(np.mean(seg_f0[seg_f0 > 0])) if np.any(seg_f0 > 0) else 0.0
        
        return {
            "timestamp": timestamp,
            "energy": seg_energy_mean,
            "pitch": seg_pitch_mean,
            "pitch_std": seg_pitch_std,
            "is_silent": seg_is_silent,
            "is_monotone": seg_pitch_std < 15.0
        }
    except Exception:
        return {
            "timestamp": timestamp,
            "energy": 0.0,
            "pitch": 0.0,
            "pitch_std": 0.0,
            "is_silent": True,
            "is_monotone": True
        }


def run_turbo_analysis(video_path: str, temp_dir: str = None, use_gpu: bool = True) -> TurboAnalysisResult:
    """
    ì´ˆê³ ì† íƒ€ì„ë©ìŠ¤ ë¶„ì„ ë©”ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
    
    15ë¶„ ì˜ìƒì„ 60ì´ˆ ì´ë‚´ì— ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        video_path: ë¶„ì„í•  ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        temp_dir: ì„ì‹œ ìºì‹œ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ìë™ ìƒì„±)
        use_gpu: GPU ê°€ì† ì‚¬ìš© ì—¬ë¶€
        
    Returns:
        TurboAnalysisResult ê°ì²´
    """
    start_time = time.time()
    
    video_path = str(Path(video_path).resolve())
    if temp_dir is None:
        temp_dir = os.path.join(os.path.dirname(video_path), ".turbo_cache")
    
    print("=" * 60)
    print("  âš¡ TURBO MODE v2: ì´ˆê³ ì† íƒ€ì„ë©ìŠ¤ ë¶„ì„")
    print("  GPU ê°€ì† + ìµœì í™”ëœ ì²­í¬ + ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸")
    print("=" * 60)
    print(f"ğŸ“ ì…ë ¥: {os.path.basename(video_path)}")
    
    # [Step 1] ë¦¬ì†ŒìŠ¤ ì¶”ì¶œ (GPU ê°€ì† ì‹œë„)
    images, audio_path = flash_extract_resources(video_path, temp_dir, use_gpu=use_gpu)
    extract_time = time.time() - start_time
    print(f"   â±ï¸ ì¶”ì¶œ ì‹œê°„: {extract_time:.1f}ì´ˆ")
    
    if not images:
        raise ValueError("í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨: ì´ë¯¸ì§€ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # [Step 2] ìµœì  ì²­í¬ í¬ê¸° ê³„ì‚°
    num_cores = multiprocessing.cpu_count()
    total_images = len(images)
    
    # ìµœì  ì²­í¬ í¬ê¸°: ì½”ì–´ë‹¹ 50-100ì¥ì´ ê°€ì¥ íš¨ìœ¨ì 
    # (ëª¨ë¸ ë¡œë”© ì˜¤ë²„í—¤ë“œ vs ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê· í˜•)
    optimal_chunk_size = max(50, min(100, total_images // num_cores))
    
    # ì²­í¬ ìˆ˜ê°€ ì½”ì–´ ìˆ˜ì˜ 1.5~2ë°°ê°€ ë˜ë„ë¡ ì¡°ì • (ë¶€í•˜ ê· í˜•)
    target_chunks = int(num_cores * 1.5)
    if total_images > target_chunks * optimal_chunk_size:
        optimal_chunk_size = total_images // target_chunks + 1
    
    image_chunks = [images[i:i + optimal_chunk_size] for i in range(0, total_images, optimal_chunk_size)]
    
    print(f"\nâš¡ [Phase 2] ë³‘ë ¬ ë¶„ì„ ì‹œì‘...")
    print(f"   ì½”ì–´: {num_cores}ê°œ, ì²­í¬: {len(image_chunks)}ê°œ (ì²­í¬ë‹¹ ~{optimal_chunk_size}ì¥)")
    
    # [Step 3] ë³‘ë ¬ ì‹¤í–‰ (Visionê³¼ Audioê°€ ë™ì‹œì— ëŒì•„ê°)
    analysis_start = time.time()
    
    with multiprocessing.Pool(processes=num_cores) as pool:
        # A. ì˜¤ë””ì˜¤ ë¶„ì„ì„ ë¹„ë™ê¸°(Async)ë¡œ ë˜ì§ (ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì„ë¼ì¸ í¬í•¨)
        audio_job = pool.apply_async(analyze_audio_track, (audio_path, 10.0))
        
        # B. ë¹„ì „ ë¶„ì„ì„ ë³‘ë ¬(Map)ë¡œ ìˆ˜í–‰
        vision_results_list = pool.map(analyze_vision_batch, image_chunks)
        
        # C. ì˜¤ë””ì˜¤ ê²°ê³¼ íšŒìˆ˜ (metrics, timeline)
        audio_result = audio_job.get()
        audio_metrics, audio_timeline = audio_result
    
    analysis_time = time.time() - analysis_start
    print(f"   â±ï¸ ë¶„ì„ ì‹œê°„: {analysis_time:.1f}ì´ˆ")

    # [Step 4] ê²°ê³¼ ë³‘í•© ë° ì •ë¦¬
    final_timeline = [item for sublist in vision_results_list for item in sublist]
    final_timeline.sort(key=lambda x: x['sec'])
    
    # ìºì‹œ ì‚­ì œ
    try:
        shutil.rmtree(temp_dir)
    except Exception:
        pass
    
    elapsed = time.time() - start_time
    
    print(f"\nâœ¨ ì „ì²´ ë¶„ì„ ì™„ë£Œ!")
    print(f"   ğŸ“Š Vision í”„ë ˆì„: {len(final_timeline)}ê°œ")
    print(f"   ğŸ”Š Audio ì„¸ê·¸ë¨¼íŠ¸: {len(audio_timeline)}ê°œ")
    print(f"   â±ï¸ ì´ ì†Œìš”: {elapsed:.2f}ì´ˆ")
    print("=" * 60)
    
    return TurboAnalysisResult(
        timeline=final_timeline,
        audio_metrics=audio_metrics,
        audio_timeline=audio_timeline,
        elapsed_seconds=elapsed,
        frame_count=len(final_timeline)
    )


class TimeLapseAnalyzer:
    """
    âš¡ íƒ€ì„ë©ìŠ¤ ë¶„ì„ê¸° í´ë˜ìŠ¤
    
    ê¸°ì¡´ ParallelFrameAnalyzerì™€ í˜¸í™˜ë˜ëŠ” ì¸í„°í˜ì´ìŠ¤ ì œê³µ
    """
    
    def __init__(self, temp_dir: str = None):
        """
        Args:
            temp_dir: ì„ì‹œ ìºì‹œ ë””ë ‰í† ë¦¬
        """
        self.temp_dir = temp_dir
        self.last_result: Optional[TurboAnalysisResult] = None
    
    def analyze_video(self, video_path: Path) -> Tuple[List[Dict], List[Dict]]:
        """
        ë¹„ë””ì˜¤ ë¶„ì„ (ParallelFrameAnalyzer í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤)
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            (vision_results, content_results) íŠœí”Œ
        """
        result = run_turbo_analysis(str(video_path), self.temp_dir)
        self.last_result = result
        
        # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        vision_results = []
        content_results = []
        
        for item in result.timeline:
            vision_results.append({
                "timestamp": item["timestamp"],
                "face_detected": item.get("face_detected", False),
                "eye_contact": item.get("eye_contact", False),
                "gesture_score": item.get("gesture_score", 0.0),
                "expression_score": item.get("expression_score", 50.0),
                "motion_score": 0.0
            })
            content_results.append({
                "timestamp": item["timestamp"],
                "text_density": item.get("text_density", 0),
                "text_density_score": min(10, item.get("text_density", 0) // 15 + 1),
                "complexity_score": item.get("slide_complexity", 0) * 100,
                "slide_detected": item.get("slide_complexity", 0) > 0.05,
                "brightness": 128.0,
                "speaker_visible": item.get("face_detected", False),
                "speaker_overlap": False
            })
        
        return vision_results, content_results
    
    def get_audio_metrics(self) -> Dict:
        """ë§ˆì§€ë§‰ ë¶„ì„ì˜ ì˜¤ë””ì˜¤ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        if self.last_result:
            return self.last_result.audio_metrics
        return {}
    
    def get_elapsed_time(self) -> float:
        """ë§ˆì§€ë§‰ ë¶„ì„ ì†Œìš” ì‹œê°„ ë°˜í™˜"""
        if self.last_result:
            return self.last_result.elapsed_seconds
        return 0.0
    
    def get_audio_timeline(self) -> List[Dict]:
        """ë§ˆì§€ë§‰ ë¶„ì„ì˜ ì˜¤ë””ì˜¤ ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì„ë¼ì¸ ë°˜í™˜"""
        if self.last_result:
            return self.last_result.audio_timeline
        return []


# ---------------------------------------------------------
# CLI í…ŒìŠ¤íŠ¸
# ---------------------------------------------------------
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        video = sys.argv[1]
    else:
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ê²½ë¡œ
        video = r"D:\data science\02.21\ë…¹í™”_2025_02_21_08_37_50_910.mp4"
    
    if os.path.exists(video):
        result = run_turbo_analysis(video)
        print(f"\nğŸ“Š íƒ€ì„ë¼ì¸ í•­ëª©: {len(result.timeline)}ê°œ")
        print(f"ğŸ”Š ì˜¤ë””ì˜¤ ë©”íŠ¸ë¦­: {result.audio_metrics}")
        print(f"â±ï¸ ëª©í‘œ 60ì´ˆ ëŒ€ë¹„: {result.elapsed_seconds/60*100:.1f}%")
    else:
        print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video}")
